{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "    \n",
    "The goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics.\n",
    "\n",
    "- The **training data** is a single, continuous segment of experimental data. \n",
    "- The **test data** consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n",
    "\n",
    "For each *seg_id* in the test folder, you should predict a single *time_to_failure* corresponding to the time between the last row of the segment and the next laboratory earthquake.\n",
    "\n",
    "[Scores](https://docs.google.com/spreadsheets/d/1AWLPZfryFWx6wM9fxmMmGm31R4B9PMrZzLp71ZtlcRk/edit#gid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Convenience\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Machine Learning libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, roc_curve, auc\n",
    "\n",
    "# Signal processing\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, CuDNNGRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "pd.options.display.precision = 15\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.width = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Earthquakes FE. More features and samples](https://www.kaggle.com/artgor/earthquakes-fe-more-features-and-samples)<br/>\n",
    "[Seismic data EDA and baseline](https://www.kaggle.com/artgor/seismic-data-eda-and-baseline)<br/>\n",
    "[Shaking Earth](https://www.kaggle.com/allunia/shaking-earth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  1.72 MB\n",
      "Reduced memory usage:  1.14 MB (33.3% reduction)\n",
      "300,000 records and 2 features in train set.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32}) # nrows=300_000\n",
    "df_train = reduce_memory_usage(df_train)\n",
    "print(\"{:,} records and {} features in train set.\".format(df_train.shape[0], df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Inspired by [Andrews Script plus a Genetic Program Model](https://www.kaggle.com/scirpus/andrews-script-plus-a-genetic-program-model)<br/>\n",
    "[Understanding and parameter setting of STA/LTA trigger\n",
    "algorithm](http://gfzpublic.gfz-potsdam.de/pubman/item/escidoc:4097:3/component/escidoc:4098/IS_8.1_rev1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_trend_feature(arr, abs_values=False):\n",
    "    idx = np.array(range(len(arr)))\n",
    "    if abs_values:\n",
    "        arr = np.abs(arr)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(idx.reshape(-1, 1), arr)\n",
    "    return lr.coef_[0]\n",
    "\n",
    "def classic_sta_lta(x, length_sta, length_lta):\n",
    "    sta = np.cumsum(x ** 2)\n",
    "\n",
    "    # Convert to float\n",
    "    sta = np.require(sta, dtype=np.float)\n",
    "\n",
    "    # Copy for LTA\n",
    "    lta = sta.copy()\n",
    "\n",
    "    # Compute the STA and the LTA\n",
    "    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n",
    "    sta /= length_sta\n",
    "    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n",
    "    lta /= length_lta\n",
    "\n",
    "    # Pad zeros\n",
    "    sta[:length_lta - 1] = 0\n",
    "\n",
    "    # Avoid division by zero by setting zero values to tiny float\n",
    "    dtiny = np.finfo(0.0).tiny\n",
    "    idx = lta < dtiny\n",
    "    lta[idx] = dtiny\n",
    "\n",
    "    return sta / lta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df_input, segment_size = 150_000):\n",
    "    segments = int(np.floor(df_input.shape[0] / segment_size))\n",
    "    print('Number of segments: {:,}'.format(segments))\n",
    "\n",
    "    X_out = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "    y_out = pd.DataFrame(index=range(segments), dtype=np.float64)\n",
    "\n",
    "    for segment in tqdm_notebook(range(segments)):\n",
    "        seg = df_input.iloc[segment*segment_size:segment*segment_size+segment_size]\n",
    "        x = pd.Series(seg['acoustic_data'].values)\n",
    "        if seg['time_to_failure']:\n",
    "            y_out.loc[segment, 'time_to_failure'] = seg['time_to_failure'].values[-1]\n",
    "\n",
    "        X_out.loc[segment, 'mean'] = x.mean()\n",
    "        X_out.loc[segment, 'std'] = x.std()\n",
    "        X_out.loc[segment, 'max'] = x.max()\n",
    "        X_out.loc[segment, 'min'] = x.min()\n",
    "\n",
    "        X_out.loc[segment, 'mean_change_abs'] = np.mean(np.diff(x))\n",
    "        X_out.loc[segment, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n",
    "        X_out.loc[segment, 'abs_max'] = np.abs(x).max()\n",
    "        X_out.loc[segment, 'abs_min'] = np.abs(x).min()\n",
    "\n",
    "        X_out.loc[segment, 'std_first_50000'] = x[:50000].std()\n",
    "        X_out.loc[segment, 'std_last_50000'] = x[-50000:].std()\n",
    "        X_out.loc[segment, 'std_first_10000'] = x[:10000].std()\n",
    "        X_out.loc[segment, 'std_last_10000'] = x[-10000:].std()\n",
    "\n",
    "        X_out.loc[segment, 'avg_first_50000'] = x[:50000].mean()\n",
    "        X_out.loc[segment, 'avg_last_50000'] = x[-50000:].mean()\n",
    "        X_out.loc[segment, 'avg_first_10000'] = x[:10000].mean()\n",
    "        X_out.loc[segment, 'avg_last_10000'] = x[-10000:].mean()\n",
    "\n",
    "        X_out.loc[segment, 'min_first_50000'] = x[:50000].min()\n",
    "        X_out.loc[segment, 'min_last_50000'] = x[-50000:].min()\n",
    "        X_out.loc[segment, 'min_first_10000'] = x[:10000].min()\n",
    "        X_out.loc[segment, 'min_last_10000'] = x[-10000:].min()\n",
    "\n",
    "        X_out.loc[segment, 'max_first_50000'] = x[:50000].max()\n",
    "        X_out.loc[segment, 'max_last_50000'] = x[-50000:].max()\n",
    "        X_out.loc[segment, 'max_first_10000'] = x[:10000].max()\n",
    "        X_out.loc[segment, 'max_last_10000'] = x[-10000:].max()\n",
    "\n",
    "        X_out.loc[segment, 'max_to_min'] = x.max() / np.abs(x.min())\n",
    "        X_out.loc[segment, 'max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "        X_out.loc[segment, 'count_big'] = len(x[np.abs(x) > 500])\n",
    "        X_out.loc[segment, 'sum'] = x.sum()\n",
    "\n",
    "        X_out.loc[segment, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(x[:50000]) / x[:50000][:-1]))[0])\n",
    "        X_out.loc[segment, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(x[-50000:]) / x[-50000:][:-1]))[0])\n",
    "        X_out.loc[segment, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(x[:10000]) / x[:10000][:-1]))[0])\n",
    "        X_out.loc[segment, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(x[-10000:]) / x[-10000:][:-1]))[0])\n",
    "\n",
    "        X_out.loc[segment, 'q95'] = np.quantile(x, 0.95)\n",
    "        X_out.loc[segment, 'q99'] = np.quantile(x, 0.99)\n",
    "        X_out.loc[segment, 'q05'] = np.quantile(x, 0.05)\n",
    "        X_out.loc[segment, 'q01'] = np.quantile(x, 0.01)\n",
    "\n",
    "        X_out.loc[segment, 'abs_q95'] = np.quantile(np.abs(x), 0.95)\n",
    "        X_out.loc[segment, 'abs_q99'] = np.quantile(np.abs(x), 0.99)\n",
    "        X_out.loc[segment, 'abs_q05'] = np.quantile(np.abs(x), 0.05)\n",
    "        X_out.loc[segment, 'abs_q01'] = np.quantile(np.abs(x), 0.01)\n",
    "\n",
    "        X_out.loc[segment, 'trend'] = add_trend_feature(x)\n",
    "        X_out.loc[segment, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "        X_out.loc[segment, 'abs_mean'] = np.abs(x).mean()\n",
    "        X_out.loc[segment, 'abs_std'] = np.abs(x).std()\n",
    "\n",
    "        X_out.loc[segment, 'mad'] = x.mad()\n",
    "        X_out.loc[segment, 'kurt'] = x.kurtosis()\n",
    "        X_out.loc[segment, 'skew'] = x.skew()\n",
    "        X_out.loc[segment, 'med'] = x.median()\n",
    "\n",
    "        X_out.loc[segment, 'Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "        X_out.loc[segment, 'Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "        X_out.loc[segment, 'classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "        X_out.loc[segment, 'classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "        X_out.loc[segment, 'classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "        X_out.loc[segment, 'classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "        X_out.loc[segment, 'Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n",
    "        X_out.loc[segment, 'Moving_average_1500_mean'] = x.rolling(window=1500).mean().mean(skipna=True)\n",
    "        X_out.loc[segment, 'Moving_average_3000_mean'] = x.rolling(window=3000).mean().mean(skipna=True)\n",
    "        X_out.loc[segment, 'Moving_average_6000_mean'] = x.rolling(window=6000).mean().mean(skipna=True)\n",
    "        ewma = pd.Series.ewm\n",
    "        X_out.loc[segment, 'exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n",
    "        X_out.loc[segment, 'exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n",
    "        X_out.loc[segment, 'exp_Moving_average_30000_mean'] = ewma(x, span=6000).mean().mean(skipna=True)\n",
    "        no_of_std = 2\n",
    "        X_out.loc[segment, 'MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n",
    "        X_out.loc[segment,'MA_700MA_BB_high_mean'] = (X_out.loc[segment, 'Moving_average_700_mean'] + no_of_std * X_out.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "        X_out.loc[segment,'MA_700MA_BB_low_mean'] = (X_out.loc[segment, 'Moving_average_700_mean'] - no_of_std * X_out.loc[segment, 'MA_700MA_std_mean']).mean()\n",
    "        X_out.loc[segment, 'MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n",
    "        X_out.loc[segment,'MA_400MA_BB_high_mean'] = (X_out.loc[segment, 'Moving_average_700_mean'] + no_of_std * X_out.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "        X_out.loc[segment,'MA_400MA_BB_low_mean'] = (X_out.loc[segment, 'Moving_average_700_mean'] - no_of_std * X_out.loc[segment, 'MA_400MA_std_mean']).mean()\n",
    "        X_out.loc[segment, 'MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n",
    "\n",
    "        X_out.loc[segment, 'iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "        X_out.loc[segment, 'q999'] = np.quantile(x, 0.999)\n",
    "        X_out.loc[segment, 'q001'] = np.quantile(x, 0.001)\n",
    "        X_out.loc[segment, 'ave10'] = stats.trim_mean(x, 0.1)\n",
    "\n",
    "        for windows in [10, 100, 1000]:\n",
    "            x_roll_std = x.rolling(windows).std().dropna().values\n",
    "            x_roll_mean = x.rolling(windows).mean().dropna().values\n",
    "\n",
    "            X_out.loc[segment, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "            X_out.loc[segment, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "            X_out.loc[segment, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "            X_out.loc[segment, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "            X_out.loc[segment, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "            X_out.loc[segment, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "            X_out.loc[segment, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "            X_out.loc[segment, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "            X_out.loc[segment, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "            X_out.loc[segment, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "            X_out.loc[segment, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "\n",
    "            X_out.loc[segment, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "            X_out.loc[segment, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "            X_out.loc[segment, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "            X_out.loc[segment, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "            X_out.loc[segment, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "            X_out.loc[segment, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "            X_out.loc[segment, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "            X_out.loc[segment, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "            X_out.loc[segment, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "            X_out.loc[segment, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "            X_out.loc[segment, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "    \n",
    "    return X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d66dcfa08d48a9947bd2f2520c1ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tr, y_tr = extract_features(df_train)\n",
    "print('{:,} samples in new train data and {:,} columns.'.format(X_tr.shape[0], X_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv', index_col='seg_id')\n",
    "X_te, _ = extract_features(submission)\n",
    "\n",
    "'''\n",
    "X_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\n",
    "\n",
    "for i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n",
    "    seg = pd.read_csv('input/test/' + seg_id + '.csv')\n",
    "    \n",
    "    x = pd.Series(seg['acoustic_data'].values)\n",
    "    X_test.loc[seg_id, 'mean'] = x.mean()\n",
    "    X_test.loc[seg_id, 'std'] = x.std()\n",
    "    X_test.loc[seg_id, 'max'] = x.max()\n",
    "    X_test.loc[seg_id, 'min'] = x.min()\n",
    "        \n",
    "    X_test.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(x))\n",
    "    X_test.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(x) / x[:-1]))[0])\n",
    "    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n",
    "    X_test.loc[seg_id, 'abs_min'] = np.abs(x).min()\n",
    "    \n",
    "    X_test.loc[seg_id, 'std_first_50000'] = x[:50000].std()\n",
    "    X_test.loc[seg_id, 'std_last_50000'] = x[-50000:].std()\n",
    "    X_test.loc[seg_id, 'std_first_10000'] = x[:10000].std()\n",
    "    X_test.loc[seg_id, 'std_last_10000'] = x[-10000:].std()\n",
    "    \n",
    "    X_test.loc[seg_id, 'avg_first_50000'] = x[:50000].mean()\n",
    "    X_test.loc[seg_id, 'avg_last_50000'] = x[-50000:].mean()\n",
    "    X_test.loc[seg_id, 'avg_first_10000'] = x[:10000].mean()\n",
    "    X_test.loc[seg_id, 'avg_last_10000'] = x[-10000:].mean()\n",
    "    \n",
    "    X_test.loc[seg_id, 'min_first_50000'] = x[:50000].min()\n",
    "    X_test.loc[seg_id, 'min_last_50000'] = x[-50000:].min()\n",
    "    X_test.loc[seg_id, 'min_first_10000'] = x[:10000].min()\n",
    "    X_test.loc[seg_id, 'min_last_10000'] = x[-10000:].min()\n",
    "    \n",
    "    X_test.loc[seg_id, 'max_first_50000'] = x[:50000].max()\n",
    "    X_test.loc[seg_id, 'max_last_50000'] = x[-50000:].max()\n",
    "    X_test.loc[seg_id, 'max_first_10000'] = x[:10000].max()\n",
    "    X_test.loc[seg_id, 'max_last_10000'] = x[-10000:].max()\n",
    "    \n",
    "    X_test.loc[seg_id, 'max_to_min'] = x.max() / np.abs(x.min())\n",
    "    X_test.loc[seg_id, 'max_to_min_diff'] = x.max() - np.abs(x.min())\n",
    "    X_test.loc[seg_id, 'count_big'] = len(x[np.abs(x) > 500])\n",
    "    X_test.loc[seg_id, 'sum'] = x.sum()\n",
    "    \n",
    "    X_test.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(x[:50000]) / x[:50000][:-1]))[0])\n",
    "    X_test.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(x[-50000:]) / x[-50000:][:-1]))[0])\n",
    "    X_test.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(x[:10000]) / x[:10000][:-1]))[0])\n",
    "    X_test.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(x[-10000:]) / x[-10000:][:-1]))[0])\n",
    "    \n",
    "    X_test.loc[seg_id, 'q95'] = np.quantile(x,0.95)\n",
    "    X_test.loc[seg_id, 'q99'] = np.quantile(x,0.99)\n",
    "    X_test.loc[seg_id, 'q05'] = np.quantile(x,0.05)\n",
    "    X_test.loc[seg_id, 'q01'] = np.quantile(x,0.01)\n",
    "    \n",
    "    X_test.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(x), 0.95)\n",
    "    X_test.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(x), 0.99)\n",
    "    X_test.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(x), 0.05)\n",
    "    X_test.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(x), 0.01)\n",
    "    \n",
    "    X_test.loc[seg_id, 'trend'] = add_trend_feature(x)\n",
    "    X_test.loc[seg_id, 'abs_trend'] = add_trend_feature(x, abs_values=True)\n",
    "    X_test.loc[seg_id, 'abs_mean'] = np.abs(x).mean()\n",
    "    X_test.loc[seg_id, 'abs_std'] = np.abs(x).std()\n",
    "    \n",
    "    X_test.loc[seg_id, 'mad'] = x.mad()\n",
    "    X_test.loc[seg_id, 'kurt'] = x.kurtosis()\n",
    "    X_test.loc[seg_id, 'skew'] = x.skew()\n",
    "    X_test.loc[seg_id, 'med'] = x.median()\n",
    "    \n",
    "    X_test.loc[seg_id, 'Hilbert_mean'] = np.abs(hilbert(x)).mean()\n",
    "    X_test.loc[seg_id, 'Hann_window_mean'] = (convolve(x, hann(150), mode='same') / sum(hann(150))).mean()\n",
    "    X_test.loc[seg_id, 'classic_sta_lta1_mean'] = classic_sta_lta(x, 500, 10000).mean()\n",
    "    X_test.loc[seg_id, 'classic_sta_lta2_mean'] = classic_sta_lta(x, 5000, 100000).mean()\n",
    "    X_test.loc[seg_id, 'classic_sta_lta3_mean'] = classic_sta_lta(x, 3333, 6666).mean()\n",
    "    X_test.loc[seg_id, 'classic_sta_lta4_mean'] = classic_sta_lta(x, 10000, 25000).mean()\n",
    "    X_test.loc[seg_id, 'Moving_average_700_mean'] = x.rolling(window=700).mean().mean(skipna=True)\n",
    "    X_test.loc[seg_id, 'Moving_average_1500_mean'] = x.rolling(window=1500).mean().mean(skipna=True)\n",
    "    X_test.loc[seg_id, 'Moving_average_3000_mean'] = x.rolling(window=3000).mean().mean(skipna=True)\n",
    "    X_test.loc[seg_id, 'Moving_average_6000_mean'] = x.rolling(window=6000).mean().mean(skipna=True)\n",
    "    ewma = pd.Series.ewm\n",
    "    X_test.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(x, span=300).mean()).mean(skipna=True)\n",
    "    X_test.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(x, span=3000).mean().mean(skipna=True)\n",
    "    X_test.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(x, span=6000).mean().mean(skipna=True)\n",
    "    no_of_std = 2\n",
    "    X_test.loc[seg_id, 'MA_700MA_std_mean'] = x.rolling(window=700).std().mean()\n",
    "    X_test.loc[seg_id,'MA_700MA_BB_high_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X_test.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X_test.loc[seg_id,'MA_700MA_BB_low_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X_test.loc[seg_id, 'MA_700MA_std_mean']).mean()\n",
    "    X_test.loc[seg_id, 'MA_400MA_std_mean'] = x.rolling(window=400).std().mean()\n",
    "    X_test.loc[seg_id,'MA_400MA_BB_high_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X_test.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X_test.loc[seg_id,'MA_400MA_BB_low_mean'] = (X_test.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X_test.loc[seg_id, 'MA_400MA_std_mean']).mean()\n",
    "    X_test.loc[seg_id, 'MA_1000MA_std_mean'] = x.rolling(window=1000).std().mean()\n",
    "    \n",
    "    X_test.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(x, [75, 25]))\n",
    "    X_test.loc[seg_id, 'q999'] = np.quantile(x,0.999)\n",
    "    X_test.loc[seg_id, 'q001'] = np.quantile(x,0.001)\n",
    "    X_test.loc[seg_id, 'ave10'] = stats.trim_mean(x, 0.1)\n",
    "    \n",
    "    for windows in [10, 100, 1000]:\n",
    "        x_roll_std = x.rolling(windows).std().dropna().values\n",
    "        x_roll_mean = x.rolling(windows).mean().dropna().values\n",
    "        \n",
    "        X_test.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n",
    "        X_test.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n",
    "        X_test.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n",
    "        X_test.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n",
    "        X_test.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n",
    "        X_test.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) / x_roll_std[:-1]))[0])\n",
    "        X_test.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n",
    "        \n",
    "        X_test.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n",
    "        X_test.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n",
    "        X_test.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n",
    "        X_test.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n",
    "        X_test.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n",
    "        X_test.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n",
    "        X_test.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n",
    "        X_test.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n",
    "        X_test.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n",
    "        X_test.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) / x_roll_mean[:-1]))[0])\n",
    "        X_test.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_all = pd.concat([X_tr, y_tr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_all = reduce_memory_usage(X_tr_all)\n",
    "\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_all.to_csv('preprocessed/segment_features_train_150k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(44, 24))\n",
    "cols = list(np.abs(X_tr.corrwith(y_tr['time_to_failure'])).sort_values(ascending=False).head(24).index)\n",
    "for i, col in enumerate(cols):\n",
    "    plt.subplot(6, 4, i + 1)\n",
    "    plt.plot(X_tr[col], color='blue')\n",
    "    plt.title(col)\n",
    "    ax1.set_ylabel(col, color='b')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    plt.plot(y_tr, color='g')\n",
    "    ax2.set_ylabel('time_to_failure', color='g')\n",
    "    plt.legend([col, 'time_to_failure'], loc=(0.875, 0.9))\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('preprocessed/segment_features_test_150k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  4.48 MB\n",
      "Reduced memory usage:  1.21 MB (73.0% reduction)\n",
      "4,194 records and 139 features in train set.\n"
     ]
    }
   ],
   "source": [
    "X_tr = pd.read_csv('preprocessed/segment_features_train_150k.csv', index_col=0)\n",
    "X_tr = reduce_memory_usage(X_tr)\n",
    "print(\"{:,} records and {} features in train set.\".format(X_tr.shape[0], X_tr.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  2.78 MB\n",
      "Reduced memory usage:  0.75 MB (73.0% reduction)\n",
      "2,624 records and 138 features in train set.\n"
     ]
    }
   ],
   "source": [
    "X_test = pd.read_csv('preprocessed/segment_features_test_150k.csv', index_col=0)\n",
    "X_test = reduce_memory_usage(X_test)\n",
    "print(\"{:,} records and {} features in train set.\".format(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_tr['time_to_failure']\n",
    "X_train = X_tr.drop('time_to_failure', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "- **Reduces Overfitting**: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- **Improves Accuracy**: Less misleading data means modeling accuracy improves.\n",
    "- **Reduces Training Time**: Less data means that algorithms train faster.\n",
    "### SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SelectKBest(score_func=mutual_info_regression, k=5)\n",
    "fit = test.fit(X_train, y_train)\n",
    "\n",
    "features = fit.transform(X_train)\n",
    "mask = fit.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data={\n",
    "    'feature_name': list(X_train.columns[mask]),\n",
    "    'score': fit.scores_[mask]\n",
    "}).sort_values(by=['score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA = [\n",
    "    # Linear models\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.Ridge(),\n",
    "    linear_model.Lasso(alpha=0.1),\n",
    "    linear_model.ElasticNet(),\n",
    "    linear_model.LassoLars(alpha=.1),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.RANSACRegressor(),\n",
    "    #linear_model.TheilSenRegressor(),\n",
    "    linear_model.HuberRegressor(),\n",
    "\n",
    "    # SVR\n",
    "    NuSVR(),\n",
    "\n",
    "    # LightGBM\n",
    "    lgb.LGBMRegressor(learning_rate=0.05, n_estimators = 50000, eval_metric='mae', early_stopping_rounds=200),\n",
    "    \n",
    "    # CatBoost\n",
    "    CatBoostRegressor(iterations=3000, learning_rate=0.03, eval_metric='MAE', verbose=False)\n",
    "]\n",
    "\n",
    "cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .25, train_size = .75, random_state = 0 )\n",
    "\n",
    "#create table to compare MLA metrics\n",
    "MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n",
    "MLA_compare = pd.DataFrame(columns = MLA_columns)\n",
    "\n",
    "#create table to compare MLA predictions\n",
    "MLA_predict = X_train\n",
    "\n",
    "#index through MLA and save performance to table\n",
    "row_index = 0\n",
    "for alg in MLA:\n",
    "    #set name and parameters\n",
    "    MLA_name = alg.__class__.__name__\n",
    "    print(MLA_name)\n",
    "    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n",
    "    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n",
    "    \n",
    "    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n",
    "    cv_results = model_selection.cross_validate(alg, X_train_scaled, y_train, cv  = cv_split)\n",
    "\n",
    "    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n",
    "    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n",
    "    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n",
    "\n",
    "    #save MLA predictions - see section 6 for usage\n",
    "    alg.fit(X_train_scaled, y_train)\n",
    "    MLA_predict[MLA_name] = alg.predict(X_train_scaled)\n",
    "\n",
    "    row_index+=1\n",
    "    \n",
    "MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLA_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, palette=sns.cubehelix_palette(len(MLA_compare), start=2, rot=0, dark=0.2, light=.85, reverse=False))\n",
    "\n",
    "plt.title('Machine Learning Algorithm Accuracy Score \\n')\n",
    "plt.xlabel('Accuracy Score (%)')\n",
    "plt.ylabel('Algorithm')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN\n",
    "[RNN starter for huge time series](https://www.kaggle.com/mayer79/rnn-starter-for-huge-time-series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(z):\n",
    "     return np.c_[z.mean(axis=1), \n",
    "                  z.min(axis=1),\n",
    "                  z.max(axis=1),\n",
    "                  z.std(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X(x, last_index=None, n_steps=150, step_length=1000):\n",
    "    if last_index == None:\n",
    "        last_index=len(x)\n",
    "\n",
    "    assert last_index - n_steps * step_length >= 0\n",
    "\n",
    "    # Reshaping and approximate standardization with mean 5 and std 3.\n",
    "    temp = (x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1) - 5 ) / 3\n",
    "\n",
    "    # Extracts features of sequences of full length 1000, of the last 100 values and finally also \n",
    "    # of the last 10 observations. \n",
    "    return np.c_[extract_features(temp),\n",
    "                 extract_features(temp[:, -step_length // 10:]),\n",
    "                 extract_features(temp[:, -step_length // 100:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = create_X(df_train[0:150000].values).shape[1]\n",
    "print(\"Our RNN is based on {} features\".format(n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "\n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "\n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "\n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Position of second (of 16) earthquake. Used to have a clean split\n",
    "# between train and validation\n",
    "second_earthquake = 50085877\n",
    "df_train.values[second_earthquake, 1]\n",
    "\n",
    "# Initialize generators\n",
    "train_gen = generator(df_train.values, batch_size=batch_size) # Use this for better score\n",
    "# train_gen = generator(float_data, batch_size=batch_size, min_index=second_earthquake + 1)\n",
    "valid_gen = generator(df_train.values, batch_size=batch_size, max_index=second_earthquake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [ModelCheckpoint(\"output/rnn.hdf5\", save_best_only=True, period=3)]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(48, input_shape=(None, n_features)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile and fit model\n",
    "model.compile(optimizer=adam(lr=0.0005), loss=\"mae\")\n",
    "\n",
    "history = model.fit_generator(train_gen,\n",
    "                              steps_per_epoch=1000,\n",
    "                              epochs=30,\n",
    "                              verbose=0,\n",
    "                              callbacks=cb,\n",
    "                              validation_data=valid_gen,\n",
    "                              validation_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def perf_plot(history, what = 'loss'):\n",
    "    x = history.history[what]\n",
    "    val_x = history.history['val_' + what]\n",
    "    epochs = np.asarray(history.epoch) + 1\n",
    "    \n",
    "    plt.plot(epochs, x, 'bo', label = \"Training \" + what)\n",
    "    plt.plot(epochs, val_x, 'b', label = \"Validation \" + what)\n",
    "    plt.title(\"Training and validation \" + what)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "perf_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\n",
    "\n",
    "# Load each test data, create the feature matrix, get numeric prediction\n",
    "for i, seg_id in enumerate(tqdm(submission.index)):\n",
    "  #  print(i)\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    x = seg['acoustic_data'].values\n",
    "    submission.time_to_failure[i] = model.predict(np.expand_dims(create_X(x), 0))\n",
    "\n",
    "submission.head()\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/jsaguiar/seismic-data-exploration\n",
    "- https://www.kaggle.com/mjbahmani/probability-of-earthquake\n",
    "- Feature selection\n",
    "- RNN\n",
    "- Ensembling (stacking, blending)\n",
    "\n",
    "https://www.kaggle.com/scirpus/andrews-script-plus-a-genetic-program-model\n",
    "https://www.kaggle.com/gpreda/lanl-earthquake-eda-and-prediction\n",
    "https://www.kaggle.com/nisargpatel/time-series-garch-ts-regression\n",
    "https://www.kaggle.com/zikazika/useful-new-features-and-a-optimised-model\n",
    "https://www.kaggle.com/mayer79/rnn-starter-for-huge-time-series\n",
    "https://www.kaggle.com/jsaguiar/seismic-data-exploration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
