{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant Category Recommendation - Autoencoder\n",
    "End date: _2019. february 19._<br/>\n",
    "\n",
    "This tutorial notebook is the second part of a seriers for [Elo Mechant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation) contest organized by Elo, one of the largest payment brands in Brazil. It has built partnerships with merchants in order to offer promotions or discounts to cardholders. The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signals in customer loyalty. The input files are available from the [download](https://www.kaggle.com/c/elo-merchant-category-recommendation/data) section of the contest:\n",
    "\n",
    "- **train.csv**,  **test.csv**: list of `card_ids` that can be used for training and testing\n",
    "- **historical_transactions.csv**: contains up to 3 months' worth of transactions for every card at any of the provided `merchant_ids`\n",
    "- **new_merchant_transactions.csv**: contains the transactions at new merchants (`merchant_ids` that this particular `card_id` \n",
    "has not yet visited) over a period of two months\n",
    "- **merchants.csv**: contains aggregate information for each `merchant_id` represented in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merch = pd.read_csv(\"input/merchants.csv\")\n",
    "print(\"{:,} records and {} features in merchant set.\".format(df_merch.shape[0], df_merch.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merch['category_1'] = df_merch['category_1'].map({'N': 0, 'Y': 1})\n",
    "df_merch['category_2'] = pd.to_numeric(df_merch['category_2'])\n",
    "df_merch['category_4'] = df_merch['category_4'].map({'N': 0, 'Y': 1})\n",
    "df_merch['most_recent_sales_range'] = df_merch['most_recent_sales_range'].map({'E': 0, 'D': 1, 'C': 2, 'B': 3, 'A': 4})\n",
    "df_merch['most_recent_purchases_range'] = df_merch['most_recent_purchases_range'].map({'E': 0, 'D': 1, 'C': 2, 'B': 3, 'A': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropping = ['city_id', 'state_id']\n",
    "for var in dropping:\n",
    "    df_merch = df_merch.drop(var, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merch = reduce_mem_usage(df_merch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans = pd.read_csv(\"input/trans_merch_new_agg.csv\", index_col=0)\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "\n",
    "df_hist_trans = pd.read_csv(\"input/trans_merch_hist_agg.csv\", index_col=0)\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"input/train.csv\")\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "\n",
    "df_test = pd.read_csv(\"input/test.csv\")\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "\n",
    "print(\"{:,} records and {} features in train set.\".format(df_train.shape[0], df_train.shape[1]))\n",
    "print(\"{:,} records and {} features in test set.\".format(df_test.shape[0], df_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_hist_trans, on='card_id',how='left')\n",
    "df_test = pd.merge(df_test, df_hist_trans, on='card_id',how='left')\n",
    "\n",
    "df_train = pd.merge(df_train, df_new_trans, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_new_trans, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_hist_trans\n",
    "del df_new_trans\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('According to Rice\\'s rule, the number of bins is {:.0f} (for the whole set)'.format(math.sqrt(df_train.shape[0])*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.hist(df_train['target'].values, bins=899)\n",
    "plt.title('Histogram target counts')\n",
    "plt.xlabel('Count')\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel('Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a peak around -33 and also the number of 0's are extremely high. It might be a good idea to handle them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outlier'] = np.where(df_train['target']<-30, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {:,} marked outliers in the training set.'.format(len(df_train[df_train['outlier'] == 1]['outlier'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df_train['outlier'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), [\"Normal\", \"Outlier\"])\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['first_active_month', 'hist_merchant_id_mode', 'new_merchant_id_mode', 'hist_avg_purchases_lag3_sum', 'hist_avg_purchases_lag3_mean', 'hist_avg_purchases_lag6_sum', 'hist_avg_purchases_lag6_mean', 'hist_avg_purchases_lag12_sum', 'hist_avg_purchases_lag12_mean']\n",
    "\n",
    "df_test.drop(columns=cols, axis=1, inplace=True)\n",
    "cols.append('card_id')\n",
    "df_train.drop(columns=cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of records in the training set: {:,}, in the test set: {:,}'.format(len(df_train), len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(how='any', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of records in the training set: {:,}, in the test set: {:,}'.format(len(df_train), len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in df_train.columns:\n",
    "    if f != 'outlier':\n",
    "        mean = statistics.mean(df_train[f])\n",
    "        std = statistics.stdev(df_train[f])\n",
    "    \n",
    "        df_train[f] = (df_train[f] - mean)/std\n",
    "        print('{}: {:.4f} ({:.4f})'.format(f, mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('input/train_preprocessed.csv')\n",
    "df_test.to_csv('input/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading normalized input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 1.34 s, total: 14.3 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_train = pd.read_csv(\"input/train_preprocessed.csv\")\n",
    "df_test = pd.read_csv(\"input/test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "[Combining your model with a model without outlier](https://www.kaggle.com/waitingli/combining-your-model-with-a-model-without-outlier)\n",
    "### Filtering out the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[df_train['outlier'] == 0]\n",
    "target = df_train['target']\n",
    "del df_train['target']\n",
    "features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','outlier']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with LightGBM on training set without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 25,\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.01,\n",
    "    'lambda_l1': 0.13,\n",
    "    'boosting': 'gbdt',\n",
    "    'feature_fraction': 0.85,\n",
    "    'bagging_freq':8,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'metric': 'rmse',\n",
    "    'verbosity': -1,\n",
    "    'random_state': 2333\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.439947\tvalid_1's rmse: 0.441321\n",
      "[200]\ttraining's rmse: 0.432385\tvalid_1's rmse: 0.435234\n",
      "[300]\ttraining's rmse: 0.427882\tvalid_1's rmse: 0.432426\n",
      "[400]\ttraining's rmse: 0.424587\tvalid_1's rmse: 0.430874\n",
      "[500]\ttraining's rmse: 0.421963\tvalid_1's rmse: 0.429922\n",
      "[600]\ttraining's rmse: 0.419742\tvalid_1's rmse: 0.429409\n",
      "[700]\ttraining's rmse: 0.417719\tvalid_1's rmse: 0.429075\n",
      "[800]\ttraining's rmse: 0.41592\tvalid_1's rmse: 0.428848\n",
      "[900]\ttraining's rmse: 0.414225\tvalid_1's rmse: 0.42871\n",
      "[1000]\ttraining's rmse: 0.412562\tvalid_1's rmse: 0.428595\n",
      "[1100]\ttraining's rmse: 0.411094\tvalid_1's rmse: 0.428542\n",
      "[1200]\ttraining's rmse: 0.409636\tvalid_1's rmse: 0.428484\n",
      "[1300]\ttraining's rmse: 0.408251\tvalid_1's rmse: 0.428467\n",
      "[1400]\ttraining's rmse: 0.407019\tvalid_1's rmse: 0.428449\n",
      "[1500]\ttraining's rmse: 0.405691\tvalid_1's rmse: 0.428428\n",
      "[1600]\ttraining's rmse: 0.404366\tvalid_1's rmse: 0.428403\n",
      "[1700]\ttraining's rmse: 0.403108\tvalid_1's rmse: 0.428439\n",
      "Early stopping, best iteration is:\n",
      "[1597]\ttraining's rmse: 0.404409\tvalid_1's rmse: 0.428397\n",
      "Fold 2.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.43989\tvalid_1's rmse: 0.441517\n",
      "[200]\ttraining's rmse: 0.43242\tvalid_1's rmse: 0.435257\n",
      "[300]\ttraining's rmse: 0.428026\tvalid_1's rmse: 0.432278\n",
      "[400]\ttraining's rmse: 0.424842\tvalid_1's rmse: 0.430597\n",
      "[500]\ttraining's rmse: 0.422257\tvalid_1's rmse: 0.429506\n",
      "[600]\ttraining's rmse: 0.420055\tvalid_1's rmse: 0.428818\n",
      "[700]\ttraining's rmse: 0.418114\tvalid_1's rmse: 0.428368\n",
      "[800]\ttraining's rmse: 0.416312\tvalid_1's rmse: 0.428037\n",
      "[900]\ttraining's rmse: 0.414649\tvalid_1's rmse: 0.427794\n",
      "[1000]\ttraining's rmse: 0.413089\tvalid_1's rmse: 0.427584\n",
      "[1100]\ttraining's rmse: 0.411624\tvalid_1's rmse: 0.427501\n",
      "[1200]\ttraining's rmse: 0.410241\tvalid_1's rmse: 0.427417\n",
      "[1300]\ttraining's rmse: 0.408818\tvalid_1's rmse: 0.427328\n",
      "[1400]\ttraining's rmse: 0.40749\tvalid_1's rmse: 0.427282\n",
      "[1500]\ttraining's rmse: 0.406159\tvalid_1's rmse: 0.427266\n",
      "[1600]\ttraining's rmse: 0.404855\tvalid_1's rmse: 0.427218\n",
      "[1700]\ttraining's rmse: 0.40354\tvalid_1's rmse: 0.427142\n",
      "[1800]\ttraining's rmse: 0.402266\tvalid_1's rmse: 0.427097\n",
      "[1900]\ttraining's rmse: 0.401014\tvalid_1's rmse: 0.427102\n",
      "[2000]\ttraining's rmse: 0.399801\tvalid_1's rmse: 0.427065\n",
      "[2100]\ttraining's rmse: 0.398622\tvalid_1's rmse: 0.427059\n",
      "[2200]\ttraining's rmse: 0.397335\tvalid_1's rmse: 0.427028\n",
      "[2300]\ttraining's rmse: 0.396173\tvalid_1's rmse: 0.427098\n",
      "Early stopping, best iteration is:\n",
      "[2179]\ttraining's rmse: 0.397591\tvalid_1's rmse: 0.427027\n",
      "Fold 3.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.440988\tvalid_1's rmse: 0.436693\n",
      "[200]\ttraining's rmse: 0.433321\tvalid_1's rmse: 0.430816\n",
      "[300]\ttraining's rmse: 0.428867\tvalid_1's rmse: 0.428163\n",
      "[400]\ttraining's rmse: 0.425558\tvalid_1's rmse: 0.426655\n",
      "[500]\ttraining's rmse: 0.422922\tvalid_1's rmse: 0.425744\n",
      "[600]\ttraining's rmse: 0.420679\tvalid_1's rmse: 0.4252\n",
      "[700]\ttraining's rmse: 0.418642\tvalid_1's rmse: 0.424817\n",
      "[800]\ttraining's rmse: 0.416797\tvalid_1's rmse: 0.424572\n",
      "[900]\ttraining's rmse: 0.415048\tvalid_1's rmse: 0.424433\n",
      "[1000]\ttraining's rmse: 0.413429\tvalid_1's rmse: 0.424314\n",
      "[1100]\ttraining's rmse: 0.41191\tvalid_1's rmse: 0.424257\n",
      "[1200]\ttraining's rmse: 0.410451\tvalid_1's rmse: 0.424173\n",
      "[1300]\ttraining's rmse: 0.409061\tvalid_1's rmse: 0.424116\n",
      "[1400]\ttraining's rmse: 0.407697\tvalid_1's rmse: 0.424071\n",
      "[1500]\ttraining's rmse: 0.40631\tvalid_1's rmse: 0.424055\n",
      "[1600]\ttraining's rmse: 0.40503\tvalid_1's rmse: 0.424043\n",
      "[1700]\ttraining's rmse: 0.403678\tvalid_1's rmse: 0.424016\n",
      "[1800]\ttraining's rmse: 0.402335\tvalid_1's rmse: 0.423998\n",
      "[1900]\ttraining's rmse: 0.401108\tvalid_1's rmse: 0.423987\n",
      "[2000]\ttraining's rmse: 0.399842\tvalid_1's rmse: 0.423999\n",
      "Early stopping, best iteration is:\n",
      "[1832]\ttraining's rmse: 0.401901\tvalid_1's rmse: 0.423969\n",
      "Fold 4.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.439065\tvalid_1's rmse: 0.444752\n",
      "[200]\ttraining's rmse: 0.431422\tvalid_1's rmse: 0.438912\n",
      "[300]\ttraining's rmse: 0.426909\tvalid_1's rmse: 0.436188\n",
      "[400]\ttraining's rmse: 0.423634\tvalid_1's rmse: 0.434627\n",
      "[500]\ttraining's rmse: 0.420958\tvalid_1's rmse: 0.43369\n",
      "[600]\ttraining's rmse: 0.418734\tvalid_1's rmse: 0.433101\n",
      "[700]\ttraining's rmse: 0.416703\tvalid_1's rmse: 0.43279\n",
      "[800]\ttraining's rmse: 0.414832\tvalid_1's rmse: 0.432512\n",
      "[900]\ttraining's rmse: 0.413112\tvalid_1's rmse: 0.432408\n",
      "[1000]\ttraining's rmse: 0.411541\tvalid_1's rmse: 0.432302\n",
      "[1100]\ttraining's rmse: 0.41\tvalid_1's rmse: 0.432254\n",
      "[1200]\ttraining's rmse: 0.40857\tvalid_1's rmse: 0.43222\n",
      "[1300]\ttraining's rmse: 0.407096\tvalid_1's rmse: 0.432196\n",
      "[1400]\ttraining's rmse: 0.405748\tvalid_1's rmse: 0.432162\n",
      "[1500]\ttraining's rmse: 0.404437\tvalid_1's rmse: 0.432195\n",
      "[1600]\ttraining's rmse: 0.403127\tvalid_1's rmse: 0.432175\n",
      "Early stopping, best iteration is:\n",
      "[1438]\ttraining's rmse: 0.40524\tvalid_1's rmse: 0.432143\n",
      "Fold 5.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's rmse: 0.439459\tvalid_1's rmse: 0.442887\n",
      "[200]\ttraining's rmse: 0.43187\tvalid_1's rmse: 0.436903\n",
      "[300]\ttraining's rmse: 0.427397\tvalid_1's rmse: 0.434145\n",
      "[400]\ttraining's rmse: 0.424121\tvalid_1's rmse: 0.432657\n",
      "[500]\ttraining's rmse: 0.421474\tvalid_1's rmse: 0.431764\n",
      "[600]\ttraining's rmse: 0.419217\tvalid_1's rmse: 0.431173\n",
      "[700]\ttraining's rmse: 0.417232\tvalid_1's rmse: 0.430801\n",
      "[800]\ttraining's rmse: 0.415421\tvalid_1's rmse: 0.43055\n",
      "[900]\ttraining's rmse: 0.413727\tvalid_1's rmse: 0.430388\n",
      "[1000]\ttraining's rmse: 0.412118\tvalid_1's rmse: 0.430286\n",
      "[1100]\ttraining's rmse: 0.410631\tvalid_1's rmse: 0.430177\n",
      "[1200]\ttraining's rmse: 0.40918\tvalid_1's rmse: 0.430165\n",
      "[1300]\ttraining's rmse: 0.407815\tvalid_1's rmse: 0.430095\n",
      "[1400]\ttraining's rmse: 0.406535\tvalid_1's rmse: 0.43007\n",
      "[1500]\ttraining's rmse: 0.405205\tvalid_1's rmse: 0.430022\n",
      "[1600]\ttraining's rmse: 0.403937\tvalid_1's rmse: 0.430047\n",
      "[1700]\ttraining's rmse: 0.402678\tvalid_1's rmse: 0.430013\n",
      "[1800]\ttraining's rmse: 0.401468\tvalid_1's rmse: 0.430027\n",
      "[1900]\ttraining's rmse: 0.40025\tvalid_1's rmse: 0.430049\n",
      "Early stopping, best iteration is:\n",
      "[1727]\ttraining's rmse: 0.402365\tvalid_1's rmse: 0.429993\n",
      "CV score: 0.42831 \n",
      "CPU times: user 44min 37s, sys: 33.3 s, total: 45min 11s\n",
      "Wall time: 13min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2333)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train, df_train['outlier'].values)):\n",
    "    print(\"Fold {}.\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=200)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.42831 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers = pd.DataFrame({\"card_id\": df_test[\"card_id\"].values})\n",
    "model_without_outliers.set_index(\"card_id\", inplace=True)\n",
    "model_without_outliers[\"target\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv('output/lightgbm_wo_outliers_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model For Outliers Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train['outlier']\n",
    "del df_train['outlier']\n",
    "del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\n",
    "categorical_feats = [c for c in features if 'feature_' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30, \n",
    "    'objective':'binary',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting': 'rf',\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_freq': 1,\n",
    "    'bagging_fraction': 0.9 ,\n",
    "    'bagging_seed': 11,\n",
    "    'metric': 'binary_logloss',\n",
    "    'lambda_l1': 0.1,\n",
    "    'verbosity': -1,\n",
    "    'random_state': 2333\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:1186: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/opt/conda/lib/python3.6/site-packages/lightgbm/basic.py:752: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0318888\tvalid_1's binary_logloss: 0.0361328\n",
      "[200]\ttraining's binary_logloss: 0.0318484\tvalid_1's binary_logloss: 0.0360422\n",
      "[300]\ttraining's binary_logloss: 0.0318666\tvalid_1's binary_logloss: 0.0360633\n",
      "Early stopping, best iteration is:\n",
      "[136]\ttraining's binary_logloss: 0.0318359\tvalid_1's binary_logloss: 0.0360665\n",
      "Fold 2.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0328958\tvalid_1's binary_logloss: 0.0322714\n",
      "[200]\ttraining's binary_logloss: 0.0328354\tvalid_1's binary_logloss: 0.0322384\n",
      "[300]\ttraining's binary_logloss: 0.0328441\tvalid_1's binary_logloss: 0.0322514\n",
      "Early stopping, best iteration is:\n",
      "[173]\ttraining's binary_logloss: 0.0328316\tvalid_1's binary_logloss: 0.0322262\n",
      "Fold 3.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0325969\tvalid_1's binary_logloss: 0.0346276\n",
      "[200]\ttraining's binary_logloss: 0.0325079\tvalid_1's binary_logloss: 0.0345696\n",
      "[300]\ttraining's binary_logloss: 0.0325175\tvalid_1's binary_logloss: 0.0345668\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's binary_logloss: 0.0325017\tvalid_1's binary_logloss: 0.0345553\n",
      "Fold 4.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.032378\tvalid_1's binary_logloss: 0.0350187\n",
      "[200]\ttraining's binary_logloss: 0.0323713\tvalid_1's binary_logloss: 0.0350133\n",
      "Early stopping, best iteration is:\n",
      "[15]\ttraining's binary_logloss: 0.0323965\tvalid_1's binary_logloss: 0.0349384\n",
      "Fold 5.\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's binary_logloss: 0.0321366\tvalid_1's binary_logloss: 0.0356896\n",
      "[200]\ttraining's binary_logloss: 0.0321506\tvalid_1's binary_logloss: 0.0357115\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttraining's binary_logloss: 0.0321374\tvalid_1's binary_logloss: 0.0356519\n",
      "CPU times: user 7min 33s, sys: 26.1 s, total: 7min 59s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n",
    "    print(\"Fold {}.\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.03469 \n"
     ]
    }
   ],
   "source": [
    "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_0ab67a22ab</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_130fd0cbdd</td>\n",
       "      <td>0.002417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_b709037bc5</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_ID_d27d835a9f</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_ID_2b5e3df5c2</td>\n",
       "      <td>0.001583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id    target\n",
       "0  C_ID_0ab67a22ab  0.002081\n",
       "1  C_ID_130fd0cbdd  0.002417\n",
       "2  C_ID_b709037bc5  0.002081\n",
       "3  C_ID_d27d835a9f  0.001607\n",
       "4  C_ID_2b5e3df5c2  0.001583"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_prob = pd.DataFrame({\"card_id\": df_test[\"card_id\"].values})\n",
    "df_outlier_prob[\"target\"] = predictions\n",
    "df_outlier_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_prob.to_csv('output/outlier_test_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of outliers in the training set is 0.0076% (1,125 records)\n",
      "If the test set has the same ration of outliers as training set, then 0.0076% of the test set is 941 records.\n"
     ]
    }
   ],
   "source": [
    "op = len(df_train[df_train['outlier']==1])/len(df_train)\n",
    "nr = int(op*len(df_test))\n",
    "print('The percentage of outliers in the training set is {:.4f}% ({:,} records)'.format(op, len(df_train[df_train['outlier']==1])))\n",
    "print('If the test set has the same ration of outliers as training set, then {:.4f}% of the test set is {:,} records.'.format(op, nr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_prob.sort_values(by='target', axis=0, ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.034976826592427804, 0.0013762192493320057)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_prob['target'].max(), df_outlier_prob['target'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>C_ID_fe89c1890a</td>\n",
       "      <td>0.034977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91167</th>\n",
       "      <td>C_ID_7a451830de</td>\n",
       "      <td>0.034548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85220</th>\n",
       "      <td>C_ID_6cca036355</td>\n",
       "      <td>0.030155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50372</th>\n",
       "      <td>C_ID_65e5c44c3e</td>\n",
       "      <td>0.030047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102165</th>\n",
       "      <td>C_ID_f4225643ec</td>\n",
       "      <td>0.029902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                card_id    target\n",
       "3709    C_ID_fe89c1890a  0.034977\n",
       "91167   C_ID_7a451830de  0.034548\n",
       "85220   C_ID_6cca036355  0.030155\n",
       "50372   C_ID_65e5c44c3e  0.030047\n",
       "102165  C_ID_f4225643ec  0.029902"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outlier_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_outlier_prob['card_id'][:nr]:\n",
    "    model_without_outliers.loc[i]['target'] = -33.218750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "941"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_without_outliers[model_without_outliers['target'] == -33.218750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_outliers.to_csv('output/lgbm_0.42831_normalized_added_outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123623, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_without_outliers.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
