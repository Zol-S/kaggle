{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant Category Recommendation - LynxKite feature boosting and LightGBM with StratifiedKFold\n",
    "End date: _2019. february 19._<br/>\n",
    "\n",
    "This tutorial notebook is part of a series for [Elo Mechant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation) contest organized by Elo, one of the largest payment brands in Brazil. It has built partnerships with merchants in order to offer promotions or discounts to cardholders. The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signals in customer loyalty. LynxKite does not yet support some of the data preprocessing, thus they need to be done in Python. The input files are available from the [download](https://www.kaggle.com/c/elo-merchant-category-recommendation/data) section of the contest:\n",
    "\n",
    "- **train.csv**,  **test.csv**: list of `card_ids` that can be used for training and testing\n",
    "- **historical_transactions.csv**: contains up to 3 months' worth of transactions for every card at any of the provided `merchant_ids`\n",
    "- **new_merchant_transactions.csv**: contains the transactions at new merchants (`merchant_ids` that this particular `card_id` \n",
    "has not yet visited) over a period of two months\n",
    "- **merchants.csv**: contains aggregate information for each `merchant_id` represented in the data set\n",
    "\n",
    "Inspired by [Feature engineering](https://github.com/zzsza/Play-Kaggle/blob/master/Elo-Merchant-Category-Recommendation/notebooks/03.Feature-Engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random.seed(402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Inspired by [Feature Engineering](https://github.com/zzsza/Play-Kaggle/blob/master/Elo-Merchant-Category-Recommendation/notebooks/04.Feature-Engineering_2nd.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  9.24 MB\n",
      "Reduced memory usage:  4.04 MB (56.2% reduction)\n",
      "201,917 observations and 5 features in train set.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"input/train.csv\", parse_dates=[\"first_active_month\"], index_col=\"card_id\")\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_train.shape[0], df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  4.72 MB\n",
      "Reduced memory usage:  2.24 MB (52.5% reduction)\n",
      "123,623 observations and 4 features in test set.\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"input/test.csv\", parse_dates=[\"first_active_month\"], index_col=\"card_id\")\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "print(\"{:,} observations and {} features in test set.\".format(df_test.shape[0], df_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['first_active_month'] = pd.to_datetime(df_train['first_active_month'])\n",
    "df_train['elapsed_days'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['first_active_month'] = pd.to_datetime(df_test['first_active_month'])\n",
    "df_test['elapsed_days'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 3109.54 MB\n",
      "Reduced memory usage: 1749.11 MB (43.7% reduction)\n",
      "Number of historical transactions: 29,112,361\n",
      "CPU times: user 1min 18s, sys: 49.5 s, total: 2min 7s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans = pd.read_csv('input/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "print('Number of historical transactions: {:,}'.format(len(df_hist_trans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 209.67 MB\n",
      "Reduced memory usage: 114.20 MB (45.5% reduction)\n",
      "Number of new transactions: 1,963,031\n",
      "CPU times: user 5.39 s, sys: 531 ms, total: 5.92 s\n",
      "Wall time: 6.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_new_trans = pd.read_csv('input/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "print('Number of new transactions: {:,}'.format(len(df_new_trans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authorized_flag              0\n",
       "card_id                      0\n",
       "city_id                      0\n",
       "category_1                   0\n",
       "installments                 0\n",
       "category_3               55922\n",
       "merchant_category_id         0\n",
       "merchant_id              26216\n",
       "month_lag                    0\n",
       "purchase_amount              0\n",
       "purchase_date                0\n",
       "category_2              111745\n",
       "state_id                     0\n",
       "subsector_id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_trans.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data handling (why these values?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_hist_trans, df_new_trans]:\n",
    "    df['category_2'].fillna(1.0, inplace=True)\n",
    "    df['category_3'].fillna('A', inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df, source_column, preposition):\n",
    "    df[preposition + '_year'] = df[source_column].dt.year\n",
    "    df[preposition + '_month'] = df[source_column].dt.month\n",
    "    df[preposition + '_day'] = df[source_column].dt.day\n",
    "    df[preposition + '_hour'] = df[source_column].dt.hour\n",
    "    df[preposition + '_weekofyear'] = df[source_column].dt.weekofyear\n",
    "    df[preposition + '_dayofweek'] = df[source_column].dt.dayofweek\n",
    "    df[preposition + '_weekend'] = (df[source_column].dt.weekday >=5).astype(int)\n",
    "    df[preposition + '_quarter'] = df[source_column].dt.quarter\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans['authorized_flag'] = df_hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "df_hist_trans['category_1'] = df_hist_trans['category_1'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "df_hist_trans['purchase_date'] = pd.to_datetime(df_hist_trans['purchase_date'])\n",
    "df_hist_trans = create_date_features(df_hist_trans, 'purchase_date', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['authorized_flag'] = df_new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "df_new_trans['category_1'] = df_new_trans['category_1'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "df_new_trans['purchase_date'] = pd.to_datetime(df_new_trans['purchase_date'])\n",
    "df_new_trans = create_date_features(df_new_trans, 'purchase_date', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans['month_diff'] = ((datetime.date(2018, 12, 1) - df_hist_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_hist_trans['month_diff'] += df_hist_trans['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['month_diff'] = ((datetime.date(2018, 12, 1) - df_new_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_new_trans['month_diff'] += df_new_trans['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 3748.10 MB\n",
      "Reduced memory usage: 1638.06 MB (56.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 248.99 MB\n",
      "Reduced memory usage: 106.71 MB (57.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_new_trans = reduce_mem_usage(df_new_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_transactions(df, prefix):\n",
    "    agg_funcs = {\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "\n",
    "        'card_id': ['size'],\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        \n",
    "        'installments': ['sum', 'max', 'min', 'mean', 'var'],\n",
    "\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'month_diff': ['mean'],\n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'purchase_dayofweek': ['nunique'],\n",
    "        'purchase_hour': ['nunique'],\n",
    "        'purchase_month': ['nunique'],\n",
    "        'purchase_year': ['nunique'],\n",
    "        'purchase_weekend': ['sum', 'mean'],\n",
    "        'purchase_weekofyear': ['nunique'],\n",
    "\n",
    "        'subsector_id': ['nunique']\n",
    "    }\n",
    "\n",
    "    df['category_2_mean'] = df.groupby(['category_2'])['purchase_amount'].transform('mean')\n",
    "    df['category_3_mean'] = df.groupby(['category_3'])['purchase_amount'].transform('mean')\n",
    "\n",
    "    df_agg = df.groupby('card_id').agg(agg_funcs)\n",
    "    df_agg.columns = [prefix + '_' + '_'.join(col).strip() for col in df_agg.columns.values]\n",
    "    df_agg.reset_index(drop=False, inplace=True)\n",
    "    df_agg[prefix + '_purchase_date_diff'] = (df_agg[prefix + '_purchase_date_max'] - df_agg[prefix + '_purchase_date_min']).dt.days\n",
    "    df_agg[prefix + '_purchase_date_average'] = df_agg[prefix + '_purchase_date_diff']/df_agg[prefix + '_card_id_size']\n",
    "    df_agg[prefix + '_purchase_date_uptonow'] = (datetime.datetime.today() - df_agg[prefix + '_purchase_date_max']).dt.days\n",
    "\n",
    "    df = (df_agg.groupby('card_id').size().reset_index(name='{}_transactions_count'.format(prefix)))\n",
    "    df_agg = pd.merge(df, df_agg, on='card_id', how='left')\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_agg = aggregate_transactions(df_hist_trans, 'hist')\n",
    "df_new_agg = aggregate_transactions(df_new_trans, 'new')\n",
    "\n",
    "df_train = df_train.merge(df_hist_agg, on='card_id', how='left')\n",
    "df_train = df_train.merge(df_new_agg, on='card_id', how='left')\n",
    "\n",
    "df_test = df_test.merge(df_hist_agg, on='card_id', how='left')\n",
    "df_test = df_test.merge(df_new_agg, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_agg[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_hist_trans, df_new_trans, df_hist_agg, df_new_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hist_first_buy'] = (df_train['hist_purchase_date_min'] - df_train['first_active_month']).dt.days\n",
    "df_train['new_first_buy'] = (df_train['new_purchase_date_min'] - df_train['first_active_month']).dt.days\n",
    "\n",
    "df_test['hist_first_buy'] = (df_test['hist_purchase_date_min'] - df_test['first_active_month']).dt.days\n",
    "df_test['new_first_buy'] = (df_test['new_purchase_date_min'] - df_test['first_active_month']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['hist_purchase_date_max', 'hist_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min']:\n",
    "    df_train[f] = df_train[f].astype(np.int64) * 1e-9\n",
    "    df_test[f] = df_test[f].astype(np.int64) * 1e-9\n",
    "\n",
    "df_train['card_id_total'] = df_train['hist_card_id_size'] + df_train['new_card_id_size']\n",
    "df_test['card_id_total'] = df_test['hist_card_id_size'] + df_test['new_card_id_size']\n",
    "\n",
    "df_train['purchase_amount_total'] = df_train['hist_purchase_amount_sum'] + df_train['new_purchase_amount_sum']\n",
    "df_test['purchase_amount_total'] = df_test['hist_purchase_amount_sum'] + df_test['new_purchase_amount_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    order_label = df_train.groupby([f])['outliers'].mean()\n",
    "    df_train[f] = df_train[f].map(order_label)\n",
    "    df_test[f] = df_test[f].map(order_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features from LynxKite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_mc = pd.read_csv(\"LynxKite_export/LynxKite-modular_clusters_id_first.csv\", index_col=\"card_id\")\n",
    "df_lk_mc = reduce_mem_usage(df_lk_mc)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_lk_mc.shape[0], df_lk_mc.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_mc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_tt = pd.read_csv(\"LynxKite_export/LynxKite-target_top-target_wavg_purchase_amount-wavg_purchase_date.csv\")\n",
    "df_lk_tt = reduce_mem_usage(df_lk_tt)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_lk_tt.shape[0], df_lk_tt.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lk_tt[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(df_lk_mc, on='card_id', how='left')\n",
    "df_test = df_test.merge(df_lk_mc, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(df_lk_tt, on='card_id', how='left')\n",
    "df_test = df_test.merge(df_lk_tt, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month', 'target', 'outliers']]\n",
    "target = df_train['target']\n",
    "del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param = {\n",
    "    'num_leaves': 31,\n",
    "    'min_data_in_leaf': 30, \n",
    "    'objective':'regression',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    \"min_child_samples\": 20,\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.9 ,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"metric\": 'rmse',\n",
    "    \"lambda_l1\": 0.1, \n",
    "    \"verbosity\": -1,\n",
    "    \"nthread\": -1,\n",
    "    \"random_state\": 402\n",
    "}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=402)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train, df_train['outliers'].values)):\n",
    "    print(\"\\nFold {}.\".format(fold_))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = np.sqrt(mean_squared_error(oof, target))\n",
    "print(\"CV score: {:.6f}\".format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n",
    "        .groupby(\"Feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,25))\n",
    "sns.barplot(x=\"importance\", y=\"Feature\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\n",
    "    \"card_id\": df_test[\"card_id\"].values\n",
    "})\n",
    "\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"output/regression_with_lk_{:.6f}.csv\".format(cv_score), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Just Train Data - LGB & XGB & CatBoost w/ Blending](https://www.kaggle.com/silverstone1903/just-train-data-lgb-xgb-catboost-w-blending/data)<br/>\n",
    "[MultiModel + RIDGE + STACKING](https://www.kaggle.com/ashishpatel26/rmse-3-66-multimodel-ridge-stacking)\n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/75935"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "169.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
