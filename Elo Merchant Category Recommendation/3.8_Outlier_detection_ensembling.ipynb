{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant Category Recommendation - Outlier detection, ensembling\n",
    "End date: _2019. february 19._<br/>\n",
    "\n",
    "This tutorial notebook is part of a series for [Elo Mechant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation) contest organized by Elo, one of the largest payment brands in Brazil. It has built partnerships with merchants in order to offer promotions or discounts to cardholders. The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signals in customer loyalty. LynxKite does not yet support some of the data preprocessing, thus they need to be done in Python. The input files are available from the [download](https://www.kaggle.com/c/elo-merchant-category-recommendation/data) section of the contest:\n",
    "\n",
    "- **train.csv**,  **test.csv**: list of `card_ids` that can be used for training and testing\n",
    "- **historical_transactions.csv**: contains up to 3 months' worth of transactions for every card at any of the provided `merchant_ids`\n",
    "- **new_merchant_transactions.csv**: contains the transactions at new merchants (`merchant_ids` that this particular `card_id` \n",
    "has not yet visited) over a period of two months\n",
    "- **merchants.csv**: contains aggregate information for each `merchant_id` represented in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import calendar\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data preparation\n",
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 289.84 MB\n",
      "Reduced memory usage: 70.52 MB (75.7% reduction)\n",
      "Starting memory usage: 325.36 MB\n",
      "Reduced memory usage: 96.86 MB (70.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_new_trans = pd.read_csv(\"preprocessed/trans_merch_new_agg.csv\")\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "\n",
    "df_hist_trans = pd.read_csv(\"preprocessed/trans_merch_hist_agg.csv\")\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "df_new_trans.drop(['Unnamed: 0'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"preprocessed/train_parsed_outlier_marked.csv\", index_col=\"card_id\")\n",
    "df_test = pd.read_csv(\"preprocessed/test_parsed.csv\", index_col=\"card_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LynxKite export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_train = pd.read_csv(\"LynxKite_export/LynxKite_outlier_viral_modeling_train.csv\", index_col=\"card_id\")\n",
    "df_lk_test = pd.read_csv(\"LynxKite_export/LynxKite_outlier_viral_modeling_test.csv\", index_col=\"card_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_train.drop(['new_id', 'outlier', 'target', 'type'], inplace=True, axis=1)\n",
    "df_lk_test.drop(['new_id', 'outlier', 'target', 'type'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_lk_train, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_lk_test, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_hist_trans, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_hist_trans, on='card_id', how='left')\n",
    "\n",
    "df_train = pd.merge(df_train, df_new_trans, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_new_trans, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 212.97 MB\n",
      "Reduced memory usage: 127.67 MB (40.1% reduction)\n",
      "Starting memory usage: 128.51 MB\n",
      "Reduced memory usage: 81.70 MB (36.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier selection\n",
    "[6 Ways for Feature Selection](https://www.kaggle.com/sz8416/6-ways-for-feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove correlating features\n",
    "[Drop Highly Correlated Features](https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns2drop = ['card_id', 'outlier', 'first_active_month', 'viral_outlier_test', 'viral_outlier_train', 'viral_roles',\n",
    "                'hist_avg_purchases_lag3_sum', 'hist_avg_purchases_lag3_mean', 'hist_avg_purchases_lag6_sum',\n",
    "                'hist_avg_purchases_lag6_mean', 'hist_avg_purchases_lag12_sum', 'hist_avg_purchases_lag12_mean',\n",
    "                'viral_outlier_spread_over_iterations', 'hist_transactions_count', 'hist_purchase_year_min',\n",
    "                'new_transactions_count', 'new_authorized_flag_mean']\n",
    "\n",
    "features_train = [c for c in df_train.columns if c not in columns2drop]\n",
    "columns2drop.remove('card_id')\n",
    "features_test = [c for c in df_test.columns if c not in columns2drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train[features_train]\n",
    "df_test_clean = df_test[features_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 275)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean.shape[1], df_test_clean.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_train_clean.corr().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n",
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train_clean.drop(to_drop, axis=1)\n",
    "df_test_clean = df_test_clean.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 292 features in the original training set, 153 features in the non-correlating training set.\n",
      "There are 290 features in the original test set, 153 features in the non-correlating test set.\n"
     ]
    }
   ],
   "source": [
    "print('There are {:} features in the original training set, {:} features in the non-correlating training set.'.format(df_train.shape[1], df_train_clean.shape[1]))\n",
    "print('There are {:} features in the original test set, {:} features in the non-correlating test set.'.format(df_test.shape[1], df_test_clean.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = list(df_train_clean.columns)\n",
    "features_train.remove('target')\n",
    "features_test = list(df_test_clean.columns)\n",
    "features_test.remove('card_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outlier'] = np.where(df_train['target']<-30, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,207 marked outliers in the training set.\n"
     ]
    }
   ],
   "source": [
    "print('There are {:,} marked outliers in the training set.'.format(len(df_train[df_train['outlier'] == 1]['outlier'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train), len(columns2drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train.dropna(how='any', axis=0, subset=features_train)[features_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    for c in df.columns:\n",
    "        mean = statistics.mean(df[c])\n",
    "        std = statistics.stdev(df[c])\n",
    "\n",
    "        df.loc[:, c] = (df[c] - mean)/std\n",
    "        print('{}: {:.4f} ({:.4f})'.format(c, mean, std))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 148,692 records in the outlier set.\n",
      "feature_1: 3.0882 (1.1964)\n",
      "feature_2: 1.7320 (0.7500)\n",
      "feature_3: 0.5536 (0.4971)\n",
      "elapsed_days: 361.1784 (285.6945)\n",
      "year: 2016.5548 (0.7575)\n",
      "month: 7.5153 (3.3239)\n",
      "days_feature1: 1153.4269 (1084.3276)\n",
      "days_feature2: 657.5544 (707.5473)\n",
      "days_feature3: 226.3963 (318.3985)\n",
      "number_of_transactions_x: 100.5018 (112.8207)\n",
      "merch_seg_viral_outlier_average_after_iteration_1_most_common: 0.0029 (0.0322)\n",
      "merch_seg_viral_outlier_average_after_iteration_2_most_common: 0.0018 (0.0243)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_1_most_common: 0.0106 (0.0462)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_2_most_common: 0.0078 (0.0356)\n",
      "viral_outlier_after_iteration_1: 0.0063 (0.0780)\n",
      "hist_authorized_flag_mean: 0.9026 (0.1030)\n",
      "hist_active_months_lag3_mean: 2.9997 (0.0049)\n",
      "hist_active_months_lag6_mean: 5.9979 (0.0194)\n",
      "hist_active_months_lag12_mean: 11.7382 (0.3147)\n",
      "hist_avg_sales_lag3_sum: 12948.6261 (150512.9370)\n",
      "hist_avg_sales_lag3_mean: 136.0217 (2037.4198)\n",
      "hist_category_1_trans_sum: 5.9185 (18.0303)\n",
      "hist_category_1_trans_mean: 0.0739 (0.1499)\n",
      "hist_category_1_merch_sum: 12.6881 (21.6681)\n",
      "hist_category_1_merch_mean: 0.1471 (0.1639)\n",
      "hist_category_2_trans_sum: 203.2621 (312.9829)\n",
      "hist_category_2_trans_mean: 2.1702 (1.3723)\n",
      "hist_category_3_sum: 52.3921 (90.3713)\n",
      "hist_category_3_mean: 0.6337 (0.6056)\n",
      "hist_category_4_sum: 51.4956 (84.5778)\n",
      "hist_category_4_mean: 0.4955 (0.3769)\n",
      "hist_city_id_trans_nunique: 5.3662 (3.7039)\n",
      "hist_city_id_merch_nunique: 3.9245 (2.6997)\n",
      "hist_installments_sum: 63.1691 (108.1101)\n",
      "hist_installments_median: 0.5904 (0.7181)\n",
      "hist_installments_mean: 0.8157 (1.0541)\n",
      "hist_installments_max: 4.3298 (23.9897)\n",
      "hist_installments_min: -0.0002 (0.7167)\n",
      "hist_installments_std: 0.7680 (3.0399)\n",
      "hist_merchant_id_nunique: 38.4271 (32.6594)\n",
      "hist_merchant_category_id_trans_nunique: 20.2105 (11.7008)\n",
      "hist_month_lag_max: -0.1280 (0.5711)\n",
      "hist_month_lag_min: -7.5679 (3.8866)\n",
      "hist_month_lag_mean: -3.6234 (2.0842)\n",
      "hist_month_lag_var: 6.7666 (6.0624)\n",
      "hist_most_recent_sales_range_mean: 2.2870 (0.5231)\n",
      "hist_most_recent_sales_range_max: 3.9323 (0.2888)\n",
      "hist_most_recent_sales_range_min: 0.0984 (0.3648)\n",
      "hist_most_recent_sales_range_std: 1.2365 (0.2319)\n",
      "hist_most_recent_purchases_range_mean: 2.3248 (0.5259)\n",
      "hist_most_recent_purchases_range_max: 3.9300 (0.2987)\n",
      "hist_most_recent_purchases_range_min: 0.1043 (0.3649)\n",
      "hist_most_recent_purchases_range_std: 1.2230 (0.2282)\n",
      "hist_numerical_1_mean: 5.0690 (8.4783)\n",
      "hist_numerical_1_median: 0.3763 (4.4306)\n",
      "hist_numerical_1_max: 57.4094 (65.1365)\n",
      "hist_numerical_1_min: -0.0537 (0.3693)\n",
      "hist_numerical_1_std: 12.3941 (15.6450)\n",
      "hist_number_of_transactions_mean: 66249.4149 (72562.2205)\n",
      "hist_number_of_transactions_median: 9163.4047 (54909.0401)\n",
      "hist_number_of_transactions_max: 824369.5118 (457626.4339)\n",
      "hist_number_of_transactions_min: 58.5677 (4496.0412)\n",
      "hist_number_of_transactions_std: 181411.9674 (126438.6485)\n",
      "hist_state_id_trans_nunique: 2.8781 (1.5804)\n",
      "hist_state_id_merch_nunique: 2.6257 (1.2534)\n",
      "hist_subsector_id_trans_nunique: 12.3836 (5.0697)\n",
      "hist_purchase_amount_sum: 17.0450 (15642.8874)\n",
      "hist_purchase_amount_min: -0.7331 (0.0469)\n",
      "hist_purchase_year_mean: 2017.1320 (0.3385)\n",
      "hist_purchase_year_max: 2017.7626 (0.4255)\n",
      "hist_purchase_year_std: 0.3063 (0.1884)\n",
      "hist_purchase_month_mean: 6.5183 (1.8487)\n",
      "hist_purchase_month_median: 6.7262 (3.0052)\n",
      "hist_purchase_month_max: 11.2216 (1.8782)\n",
      "hist_purchase_month_min: 1.5527 (1.6563)\n",
      "hist_purchase_month_std: 3.4626 (1.1846)\n",
      "hist_purchase_month_nunique: 7.3539 (3.1372)\n",
      "hist_purchase_day_mean: 15.9699 (2.5705)\n",
      "hist_purchase_day_median: 16.0349 (3.9766)\n",
      "hist_purchase_day_max: 29.8717 (2.2059)\n",
      "hist_purchase_day_min: 2.0198 (2.2648)\n",
      "hist_purchase_day_std: 8.2777 (1.4080)\n",
      "hist_purchase_day_nunique: 21.6704 (7.9215)\n",
      "hist_purchase_hour_mean: 14.0840 (1.4969)\n",
      "hist_purchase_hour_median: 14.5944 (1.9720)\n",
      "hist_purchase_hour_max: 22.0727 (1.5082)\n",
      "hist_purchase_hour_min: 1.9255 (3.5238)\n",
      "hist_purchase_hour_std: 5.0893 (1.3081)\n",
      "hist_purchase_hour_nunique: 15.6302 (4.6701)\n",
      "hist_purchase_weekofyear_nunique: 21.4329 (12.5693)\n",
      "hist_purchase_dayofweek_mean: 3.0250 (0.5364)\n",
      "hist_purchase_dayofweek_median: 3.1317 (0.8617)\n",
      "hist_purchase_dayofweek_max: 5.8623 (0.4316)\n",
      "hist_purchase_dayofweek_min: 0.0719 (0.3354)\n",
      "hist_purchase_dayofweek_std: 1.8444 (0.2393)\n",
      "hist_purchase_dayofweek_nunique: 6.6280 (0.8351)\n",
      "hist_purchase_part_of_month_median: 1.1181 (0.4634)\n",
      "hist_purchase_weekend_sum: 28.2877 (34.2605)\n",
      "hist_purchase_weekend_mean: 0.2803 (0.1341)\n",
      "new_authorized_flag_sum: 7.9198 (6.8055)\n",
      "new_active_months_lag3_mean: 2.9998 (0.0074)\n",
      "new_active_months_lag6_mean: 5.9912 (0.0647)\n",
      "new_active_months_lag12_mean: 11.7553 (0.5119)\n",
      "new_avg_sales_lag3_sum: 365.9821 (17275.8101)\n",
      "new_avg_sales_lag3_mean: 41.5959 (2435.4469)\n",
      "new_category_1_trans_sum: 0.2269 (0.6085)\n",
      "new_category_1_trans_mean: 0.0345 (0.1005)\n",
      "new_category_1_merch_sum: 0.6810 (1.0042)\n",
      "new_category_1_merch_mean: 0.1018 (0.1619)\n",
      "new_category_2_trans_sum: 16.5252 (20.1552)\n",
      "new_category_2_trans_mean: 2.1820 (1.4010)\n",
      "new_category_3_sum: 4.5592 (6.9251)\n",
      "new_category_3_mean: 0.6195 (0.6177)\n",
      "new_category_4_sum: 3.9051 (5.1221)\n",
      "new_category_4_mean: 0.4777 (0.3891)\n",
      "new_city_id_trans_nunique: 2.5755 (1.7191)\n",
      "new_city_id_merch_nunique: 2.2956 (1.3503)\n",
      "new_installments_sum: 5.3416 (8.7643)\n",
      "new_installments_median: 0.6080 (0.8034)\n",
      "new_installments_mean: 0.7060 (0.9233)\n",
      "new_installments_max: 1.5748 (3.5711)\n",
      "new_installments_min: 0.2199 (0.7444)\n",
      "new_installments_std: 0.5138 (1.0854)\n",
      "new_merchant_category_id_trans_nunique: 6.1905 (4.1828)\n",
      "new_month_lag_max: 1.8780 (0.3273)\n",
      "new_month_lag_min: 1.0994 (0.2991)\n",
      "new_month_lag_mean: 1.4756 (0.2892)\n",
      "new_month_lag_var: 0.2095 (0.1344)\n",
      "new_most_recent_sales_range_mean: 2.0713 (0.6730)\n",
      "new_most_recent_sales_range_max: 3.4728 (0.8235)\n",
      "new_most_recent_sales_range_min: 0.6714 (0.8708)\n",
      "new_most_recent_sales_range_std: 1.1668 (0.4636)\n",
      "new_most_recent_purchases_range_mean: 2.0972 (0.6727)\n",
      "new_most_recent_purchases_range_max: 3.4790 (0.8343)\n",
      "new_most_recent_purchases_range_min: 0.7369 (0.8516)\n",
      "new_most_recent_purchases_range_std: 1.1442 (0.4568)\n",
      "new_numerical_1_mean: 3.6217 (8.4146)\n",
      "new_numerical_1_median: 0.9089 (5.6879)\n",
      "new_numerical_1_max: 19.7073 (39.0339)\n",
      "new_numerical_1_min: 0.0178 (1.4406)\n",
      "new_numerical_1_std: 7.2270 (14.6822)\n",
      "new_number_of_transactions_mean: 29647.1345 (77295.3486)\n"
     ]
    }
   ],
   "source": [
    "y = df_train_clean['outlier']\n",
    "print('There are {:,} records in the outlier set.'.format(len(y)))\n",
    "\n",
    "df_train_clean.drop(['outlier'], axis=1, inplace=True)\n",
    "X = normalize(df_train_clean)\n",
    "print('There are {:,} features and {:,} items in the training set.'.format(X.shape[1], X.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection of outlier selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y, limit=100):\n",
    "    cor_list = []\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-limit:]].columns.tolist()\n",
    "    cor_support = [True if i in cor_feature else False for i in X.columns.tolist()]\n",
    "    return cor_support, cor_feature, cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 selected features\n"
     ]
    }
   ],
   "source": [
    "cor_support, cor_feature, cor_value = cor_selector(X, y)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype float16, float32, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=100, score_func=<function chi2 at 0x7f7960863bf8>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2, k=100)\n",
    "chi_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 selected features\n"
     ]
    }
   ],
   "source": [
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(str(len(chi_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 184 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 14 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RFE(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "  n_features_to_select=5, step=10, verbose=5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=5, step=10, verbose=5)\n",
    "rfe_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 selected features\n"
     ]
    }
   ],
   "source": [
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False,\n",
       "        threshold='1.25*median')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), '1.25*median')\n",
    "embeded_lr_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 selected features\n"
     ]
    }
   ],
   "source": [
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "        max_features=None, norm_order=1, prefit=False,\n",
       "        threshold='1.25*median')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=5), threshold='1.25*median')\n",
    "embeded_rf_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 selected features\n"
     ]
    }
   ],
   "source": [
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.2,\n",
       "        importance_type='split', learning_rate=0.05, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=40, min_split_gain=0.01,\n",
       "        n_estimators=500, n_jobs=-1, num_leaves=32, objective=None,\n",
       "        random_state=None, reg_alpha=3, reg_lambda=1, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       "        max_features=None, norm_order=1, prefit=False,\n",
       "        threshold='1.25*median')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, threshold='1.25*median')\n",
    "embeded_lgb_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 selected features\n"
     ]
    }
   ],
   "source": [
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM 2.\n",
    "[Outlier handling strategies](https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/78470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_asymmetric_train(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    residualtrain: = (y_true - y_pred).astype(\"float\")\n",
    "    grad = -4*residual**3\n",
    "    hess = 12*residual**2\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "def custom_asymmetric_valid(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = residual**5\n",
    "\n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False\n",
    "\n",
    "model = lgb.train({\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'None',\n",
    "        'metric': 'None',\n",
    "        'learning_rate': 0.05\n",
    "    },\n",
    "    df_train_clean,\n",
    "    num_boost_round=10,\n",
    "    fobj=custom_asymmetric_train,\n",
    "    feval=custom_asymmetric_valid,\n",
    "    #valid_sets=eval_set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>Pearson</th>\n",
       "      <th>Chi-2</th>\n",
       "      <th>RFE</th>\n",
       "      <th>Logistics</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>viral_outlier_after_iteration_1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_purchase_weekofyear_nunique</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_purchase_quarter_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_purchase_month_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new_purchase_hour_std</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new_purchase_dayofweek_std</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>new_purchase_day_std</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>new_purchase_day_median</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>new_purchase_day_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>new_purchase_day_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>new_number_of_transactions_median</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>new_number_of_transactions_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>new_month_lag_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>merch_seg_viral_outlier_standard_deviation_aft...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>merch_seg_viral_outlier_standard_deviation_aft...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>merch_seg_viral_outlier_average_after_iteratio...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>merch_seg_viral_outlier_average_after_iteratio...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hist_purchase_year_std</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hist_purchase_year_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hist_purchase_month_std</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hist_month_lag_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hist_installments_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hist_category_4_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hist_category_3_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hist_category_1_trans_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hist_category_1_trans_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hist_category_1_merch_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hist_category_1_merch_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hist_authorized_flag_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>days_feature1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>new_purchase_part_of_month_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>new_purchase_month_min</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>new_purchase_month_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>new_purchase_hour_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>new_purchase_day_nunique</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>new_most_recent_sales_range_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>new_most_recent_purchases_range_max</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>new_month_lag_var</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>new_merchant_category_id_trans_nunique</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>new_category_2_trans_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>new_category_1_trans_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>new_category_1_trans_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>new_authorized_flag_sum</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>hist_purchase_month_nunique</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>hist_purchase_hour_nunique</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>hist_number_of_transactions_median</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>hist_most_recent_purchases_range_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>hist_month_lag_min</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>hist_installments_mean</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  Pearson  Chi-2    RFE  \\\n",
       "1                     viral_outlier_after_iteration_1     True   True   True   \n",
       "2                     new_purchase_weekofyear_nunique     True   True   True   \n",
       "3                            new_purchase_quarter_max     True   True  False   \n",
       "4                             new_purchase_month_mean     True   True  False   \n",
       "5                               new_purchase_hour_std     True   True  False   \n",
       "6                          new_purchase_dayofweek_std     True   True  False   \n",
       "7                                new_purchase_day_std     True   True  False   \n",
       "8                             new_purchase_day_median     True   True  False   \n",
       "9                               new_purchase_day_mean     True   True  False   \n",
       "10                               new_purchase_day_max     True   True  False   \n",
       "11                  new_number_of_transactions_median     True   True  False   \n",
       "12                    new_number_of_transactions_mean     True   True  False   \n",
       "13                                 new_month_lag_mean     True   True  False   \n",
       "14                                              month     True   True  False   \n",
       "15  merch_seg_viral_outlier_standard_deviation_aft...     True   True  False   \n",
       "16  merch_seg_viral_outlier_standard_deviation_aft...     True   True  False   \n",
       "17  merch_seg_viral_outlier_average_after_iteratio...     True   True  False   \n",
       "18  merch_seg_viral_outlier_average_after_iteratio...     True   True  False   \n",
       "19                             hist_purchase_year_std     True   True   True   \n",
       "20                             hist_purchase_year_max     True   True  False   \n",
       "21                            hist_purchase_month_std     True   True  False   \n",
       "22                                hist_month_lag_mean     True   True  False   \n",
       "23                              hist_installments_sum     True   True  False   \n",
       "24                               hist_category_4_mean     True   True  False   \n",
       "25                                hist_category_3_sum     True   True  False   \n",
       "26                          hist_category_1_trans_sum     True   True  False   \n",
       "27                         hist_category_1_trans_mean     True   True  False   \n",
       "28                          hist_category_1_merch_sum     True   True  False   \n",
       "29                         hist_category_1_merch_mean     True   True  False   \n",
       "30                          hist_authorized_flag_mean     True   True  False   \n",
       "31                                      days_feature1     True   True  False   \n",
       "32                    new_purchase_part_of_month_mean     True   True  False   \n",
       "33                             new_purchase_month_min     True   True  False   \n",
       "34                             new_purchase_month_max     True   True  False   \n",
       "35                             new_purchase_hour_mean     True  False  False   \n",
       "36                           new_purchase_day_nunique     True   True  False   \n",
       "37                    new_most_recent_sales_range_max     True   True  False   \n",
       "38                new_most_recent_purchases_range_max     True   True  False   \n",
       "39                                  new_month_lag_var     True   True  False   \n",
       "40             new_merchant_category_id_trans_nunique     True   True  False   \n",
       "41                           new_category_2_trans_sum     True   True  False   \n",
       "42                           new_category_1_trans_sum     True   True   True   \n",
       "43                          new_category_1_trans_mean     True   True  False   \n",
       "44                            new_authorized_flag_sum     True   True  False   \n",
       "45                        hist_purchase_month_nunique     True   True  False   \n",
       "46                         hist_purchase_hour_nunique     True   True   True   \n",
       "47                 hist_number_of_transactions_median    False   True  False   \n",
       "48              hist_most_recent_purchases_range_mean     True   True  False   \n",
       "49                                 hist_month_lag_min     True   True  False   \n",
       "50                             hist_installments_mean     True   True  False   \n",
       "\n",
       "    Logistics  Random Forest  LightGBM  total  \n",
       "1        True           True      True      6  \n",
       "2        True          False      True      5  \n",
       "3        True           True      True      5  \n",
       "4        True           True      True      5  \n",
       "5        True           True      True      5  \n",
       "6        True           True      True      5  \n",
       "7        True           True      True      5  \n",
       "8        True           True      True      5  \n",
       "9        True           True      True      5  \n",
       "10       True           True      True      5  \n",
       "11       True           True      True      5  \n",
       "12       True           True      True      5  \n",
       "13       True           True      True      5  \n",
       "14       True           True      True      5  \n",
       "15       True           True      True      5  \n",
       "16       True           True      True      5  \n",
       "17       True           True      True      5  \n",
       "18       True           True      True      5  \n",
       "19       True          False      True      5  \n",
       "20       True           True      True      5  \n",
       "21       True           True      True      5  \n",
       "22       True           True      True      5  \n",
       "23       True           True      True      5  \n",
       "24       True           True      True      5  \n",
       "25       True           True      True      5  \n",
       "26       True           True      True      5  \n",
       "27       True           True      True      5  \n",
       "28       True           True      True      5  \n",
       "29       True           True      True      5  \n",
       "30       True           True      True      5  \n",
       "31       True           True      True      5  \n",
       "32       True          False      True      4  \n",
       "33       True          False      True      4  \n",
       "34       True          False      True      4  \n",
       "35       True           True      True      4  \n",
       "36       True           True     False      4  \n",
       "37       True          False      True      4  \n",
       "38       True           True     False      4  \n",
       "39       True          False      True      4  \n",
       "40       True           True     False      4  \n",
       "41       True          False      True      4  \n",
       "42       True          False     False      4  \n",
       "43       True          False      True      4  \n",
       "44       True           True     False      4  \n",
       "45       True          False      True      4  \n",
       "46       True          False     False      4  \n",
       "47       True           True      True      4  \n",
       "48       True           True     False      4  \n",
       "49       True           True     False      4  \n",
       "50       True          False      True      4  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "feature_selection_df = pd.DataFrame({\n",
    "    'feature': X.columns.tolist(),\n",
    "    'Pearson': cor_support,\n",
    "    'Chi-2': chi_support,\n",
    "    'RFE': rfe_support,\n",
    "    'Logistics': embeded_lr_support,\n",
    "    'Random Forest': embeded_rf_support,\n",
    "    'LightGBM': embeded_lgb_support\n",
    "})\n",
    "\n",
    "feature_selection_df['total'] = np.sum(feature_selection_df, axis=1)\n",
    "\n",
    "feature_selection_df = feature_selection_df.sort_values(['total', 'feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_selection_df[feature_selection_df['total'] > 4]['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['viral_outlier_after_iteration_1',\n",
       " 'new_purchase_weekofyear_nunique',\n",
       " 'new_purchase_quarter_max',\n",
       " 'new_purchase_month_mean',\n",
       " 'new_purchase_hour_std',\n",
       " 'new_purchase_dayofweek_std',\n",
       " 'new_purchase_day_std',\n",
       " 'new_purchase_day_median',\n",
       " 'new_purchase_day_mean',\n",
       " 'new_purchase_day_max',\n",
       " 'new_number_of_transactions_median',\n",
       " 'new_number_of_transactions_mean',\n",
       " 'new_month_lag_mean',\n",
       " 'month',\n",
       " 'merch_seg_viral_outlier_standard_deviation_after_iteration_2_most_common',\n",
       " 'merch_seg_viral_outlier_standard_deviation_after_iteration_1_most_common',\n",
       " 'merch_seg_viral_outlier_average_after_iteration_2_most_common',\n",
       " 'merch_seg_viral_outlier_average_after_iteration_1_most_common',\n",
       " 'hist_purchase_year_std',\n",
       " 'hist_purchase_year_max',\n",
       " 'hist_purchase_month_std',\n",
       " 'hist_month_lag_mean',\n",
       " 'hist_installments_sum',\n",
       " 'hist_category_4_mean',\n",
       " 'hist_category_3_sum',\n",
       " 'hist_category_1_trans_sum',\n",
       " 'hist_category_1_trans_mean',\n",
       " 'hist_category_1_merch_sum',\n",
       " 'hist_category_1_merch_mean',\n",
       " 'hist_authorized_flag_mean',\n",
       " 'days_feature1']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(feature_selection_df[feature_selection_df['total'] > 4]['feature'])\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all features\n",
    "#features = [c for c in df_train.columns if c not in columns2drop]\n",
    "\n",
    "features.append('outlier')\n",
    "\n",
    "df_train_clean = df_train.dropna(how='any', axis=0, subset=features)[features]\n",
    "y = df_train_clean.outlier\n",
    "\n",
    "df_train_clean.drop(['outlier'], axis=1, inplace=True)\n",
    "X = normalize(df_train_clean)\n",
    "\n",
    "features_test = features\n",
    "features_test.remove('outlier')\n",
    "features_test.append('card_id')\n",
    "df_test_clean = df_test.dropna(how='any', axis=0, subset=features_test)[features_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, df_test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test.remove('card_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(verbose=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "randomforest_outlier_pred = clf.predict(df_test_clean[features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_outlier_card_ids = []\n",
    "for i in range(len(randomforest_outlier_pred)):\n",
    "    if randomforest_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        rf_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_outlier_card_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(verbose=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_outlier_pred = clf.predict(df_test_clean[features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,487. card_id: C_ID_c0b4e08398\n",
      "7,825. card_id: C_ID_fd158a8b90\n",
      "26,981. card_id: C_ID_6d7769f9a9\n",
      "36,010. card_id: C_ID_34aa1d9963\n",
      "45,790. card_id: C_ID_a777236765\n",
      "49,412. card_id: C_ID_6ba5524817\n",
      "51,629. card_id: C_ID_c9a8d6baa9\n",
      "55,225. card_id: C_ID_a2f2be2611\n",
      "70,382. card_id: C_ID_cdce5657d9\n",
      "71,233. card_id: C_ID_2e0ac7da4e\n",
      "90,515. card_id: C_ID_c3ed09b27d\n"
     ]
    }
   ],
   "source": [
    "lr_outlier_card_ids = []\n",
    "for i in range(len(logistic_regression_outlier_pred)):\n",
    "    if logistic_regression_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        lr_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_outlier_pred = clf.predict(df_test_clean[features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_outlier_card_ids = []\n",
    "for i in range(len(adaboost_outlier_pred)):\n",
    "    if adaboost_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        adaboost_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the output of the logistic regression: 11\n",
      "Size of the output of the random forest: 0\n",
      "Size of the output of the AdaBoost: 0\n"
     ]
    }
   ],
   "source": [
    "print('Size of the output of the logistic regression: {}\\nSize of the output of the random forest: {}\\nSize of the output of the AdaBoost: {}'.format(len(lr_outlier_card_ids), len(rf_outlier_card_ids), len(adaboost_outlier_card_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(lr_outlier_card_ids, rf_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(lr_outlier_card_ids, adaboost_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(rf_outlier_card_ids, adaboost_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "LightGBM training without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153, 153)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean.shape[1], df_test_clean.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LightGBM parameters](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst)<br/>\n",
    "[Comparison between LGB boosting methods (goss, gbdt and dart)](https://www.kaggle.com/c/home-credit-default-risk/discussion/60921)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_train_clean['target']\n",
    "df_train_clean = df_train_clean.drop(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201917, 152), (201917,), (123623, 153))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean.shape, target.shape, df_test_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df_train_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.02201\tvalid_1's rmse: 3.06874\n",
      "[200]\ttraining's rmse: 2.75042\tvalid_1's rmse: 2.81028\n",
      "[300]\ttraining's rmse: 2.50437\tvalid_1's rmse: 2.57954\n",
      "[400]\ttraining's rmse: 2.34652\tvalid_1's rmse: 2.44063\n",
      "[500]\ttraining's rmse: 2.22318\tvalid_1's rmse: 2.33729\n",
      "[600]\ttraining's rmse: 2.20662\tvalid_1's rmse: 2.32944\n",
      "[700]\ttraining's rmse: 2.13772\tvalid_1's rmse: 2.27771\n",
      "[800]\ttraining's rmse: 2.10913\tvalid_1's rmse: 2.26048\n",
      "[900]\ttraining's rmse: 2.06402\tvalid_1's rmse: 2.23272\n",
      "[1000]\ttraining's rmse: 2.04356\tvalid_1's rmse: 2.22419\n",
      "[1100]\ttraining's rmse: 2.01847\tvalid_1's rmse: 2.21462\n",
      "Early stopping, best iteration is:\n",
      "[1045]\ttraining's rmse: 2.02336\tvalid_1's rmse: 2.2135\n",
      "Fold 2.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.04181\tvalid_1's rmse: 2.98727\n",
      "[200]\ttraining's rmse: 2.77018\tvalid_1's rmse: 2.72602\n",
      "[300]\ttraining's rmse: 2.52399\tvalid_1's rmse: 2.4941\n",
      "[400]\ttraining's rmse: 2.3664\tvalid_1's rmse: 2.35489\n",
      "[500]\ttraining's rmse: 2.24176\tvalid_1's rmse: 2.2513\n",
      "[600]\ttraining's rmse: 2.22557\tvalid_1's rmse: 2.24422\n",
      "[700]\ttraining's rmse: 2.15691\tvalid_1's rmse: 2.1922\n",
      "[800]\ttraining's rmse: 2.12788\tvalid_1's rmse: 2.17518\n",
      "[900]\ttraining's rmse: 2.08253\tvalid_1's rmse: 2.1469\n",
      "[1000]\ttraining's rmse: 2.06247\tvalid_1's rmse: 2.13846\n",
      "[1100]\ttraining's rmse: 2.03695\tvalid_1's rmse: 2.12819\n",
      "Early stopping, best iteration is:\n",
      "[1045]\ttraining's rmse: 2.04219\tvalid_1's rmse: 2.12765\n",
      "Fold 3.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.03776\tvalid_1's rmse: 3.00895\n",
      "[200]\ttraining's rmse: 2.76293\tvalid_1's rmse: 2.76558\n",
      "[300]\ttraining's rmse: 2.51378\tvalid_1's rmse: 2.54891\n",
      "[400]\ttraining's rmse: 2.35477\tvalid_1's rmse: 2.42021\n",
      "[500]\ttraining's rmse: 2.2286\tvalid_1's rmse: 2.32355\n",
      "[600]\ttraining's rmse: 2.21211\tvalid_1's rmse: 2.3176\n",
      "[700]\ttraining's rmse: 2.14256\tvalid_1's rmse: 2.26963\n",
      "[800]\ttraining's rmse: 2.11361\tvalid_1's rmse: 2.25364\n",
      "[900]\ttraining's rmse: 2.06792\tvalid_1's rmse: 2.22791\n",
      "[1000]\ttraining's rmse: 2.04722\tvalid_1's rmse: 2.21999\n",
      "[1100]\ttraining's rmse: 2.02121\tvalid_1's rmse: 2.20983\n",
      "Early stopping, best iteration is:\n",
      "[1045]\ttraining's rmse: 2.02687\tvalid_1's rmse: 2.20934\n",
      "Fold 4.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 2.99453\tvalid_1's rmse: 3.17922\n",
      "[200]\ttraining's rmse: 2.7223\tvalid_1's rmse: 2.92469\n",
      "[300]\ttraining's rmse: 2.47638\tvalid_1's rmse: 2.69978\n",
      "[400]\ttraining's rmse: 2.31886\tvalid_1's rmse: 2.56565\n",
      "[500]\ttraining's rmse: 2.19455\tvalid_1's rmse: 2.46581\n",
      "[600]\ttraining's rmse: 2.17857\tvalid_1's rmse: 2.45874\n",
      "[700]\ttraining's rmse: 2.10973\tvalid_1's rmse: 2.40891\n",
      "[800]\ttraining's rmse: 2.08023\tvalid_1's rmse: 2.39324\n",
      "[900]\ttraining's rmse: 2.03619\tvalid_1's rmse: 2.36629\n",
      "[1000]\ttraining's rmse: 2.016\tvalid_1's rmse: 2.35845\n"
     ]
    }
   ],
   "source": [
    "param = {\n",
    "    'num_leaves': 127,\n",
    "    'min_data_in_leaf': 20, \n",
    "    'objective': 'regression_l2',\n",
    "    'max_depth': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    \"boosting\": \"dart\",\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"metric\": 'rmse',\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "\n",
    "oof = np.zeros(len(df_train_clean))\n",
    "predictions = np.zeros(len(df_test_clean))\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train_clean.values, target.values)):\n",
    "    print(\"Fold {}.\".format(fold_ + 1))\n",
    "    trn_data = lgb.Dataset(df_train_clean.iloc[trn_idx], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train_clean.iloc[val_idx], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n",
    "    oof[val_idx] = clf.predict(df_train_clean.iloc[val_idx], num_iteration=clf.best_iteration)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features_train\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    predictions += clf.predict(df_test[features_test], num_iteration=clf.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_lgb = np.sqrt(mean_squared_error(target, oof))\n",
    "print('Cross-validation score: ' + str(cross_validation_lgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features = feature_importance_df.loc[feature_importance_df['feature'].isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14, 40))\n",
    "sns.barplot(x=\"importance\",\n",
    "            y=\"feature\",\n",
    "            data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "df_sub[\"target\"] = predictions\n",
    "df_sub.to_csv(\"output/lgbm_{}.csv\".format(cross_validation_lgb), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Load the model that was trained on the training set without outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.Booster(model_file='models/lightgbm_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['card_id', 'first_active_month', 'target', 'outlier']\n",
    "use_cols = [c for c in df_train.columns if c not in drops]\n",
    "features = list(df_train[use_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(df_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    \"card_id\": df_test[\"card_id\"].values\n",
    "})\n",
    "df_sub[\"target\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10 x log2(10) = -33.21928095\n"
     ]
    }
   ],
   "source": [
    "print('-10 x log2(10) = {:.8f}'.format(-10*np.log2(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the intersection of Logistic regression & Random Forest\n",
    "df_sub.loc[df_sub['card_id'] == 'C_ID_aae50409e7', 'target'] = -33.218750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lr_outlier_card_ids)):\n",
    "    print('The value of {} is {}'.format(lr_outlier_card_ids[i], df_sub.loc[df_sub['card_id'] == lr_outlier_card_ids[i], 'target'].values[0]))\n",
    "    df_sub.loc[df_sub['card_id'] == lr_outlier_card_ids[i], 'target'] = -33.218750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sub[df_sub['target'] < -30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"output/lgbm_rf_and_adaboost_outliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest (LB score: 5.994)\n",
    "* Logistic Regression (LB score: 5.990)\n",
    "* Random Forest & AdaBoost (LB score: 5.986)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
