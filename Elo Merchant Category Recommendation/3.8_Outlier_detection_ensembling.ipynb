{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant Category Recommendation - Outlier detection, ensembling\n",
    "End date: _2019. february 19._<br/>\n",
    "\n",
    "This tutorial notebook is part of a series for [Elo Mechant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation) contest organized by Elo, one of the largest payment brands in Brazil. It has built partnerships with merchants in order to offer promotions or discounts to cardholders. The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signals in customer loyalty. LynxKite does not yet support some of the data preprocessing, thus they need to be done in Python. The input files are available from the [download](https://www.kaggle.com/c/elo-merchant-category-recommendation/data) section of the contest:\n",
    "\n",
    "- **train.csv**,  **test.csv**: list of `card_ids` that can be used for training and testing\n",
    "- **historical_transactions.csv**: contains up to 3 months' worth of transactions for every card at any of the provided `merchant_ids`\n",
    "- **new_merchant_transactions.csv**: contains the transactions at new merchants (`merchant_ids` that this particular `card_id` \n",
    "has not yet visited) over a period of two months\n",
    "- **merchants.csv**: contains aggregate information for each `merchant_id` represented in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "import datetime\n",
    "import calendar\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data preparation\n",
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 289.84 MB\n",
      "Reduced memory usage: 70.52 MB (75.7% reduction)\n",
      "Starting memory usage: 325.36 MB\n",
      "Reduced memory usage: 96.86 MB (70.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_new_trans = pd.read_csv(\"preprocessed/trans_merch_new_agg.csv\")\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "\n",
    "df_hist_trans = pd.read_csv(\"preprocessed/trans_merch_hist_agg.csv\")\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans.drop(['Unnamed: 0'], inplace=True, axis=1)\n",
    "df_new_trans.drop(['Unnamed: 0'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"preprocessed/train_parsed_outlier_marked.csv\", index_col=\"card_id\")\n",
    "df_test = pd.read_csv(\"preprocessed/test_parsed.csv\", index_col=\"card_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LynxKite export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_train = pd.read_csv(\"LynxKite_export/LynxKite_outlier_viral_modeling_train.csv\", index_col=\"card_id\")\n",
    "df_lk_test = pd.read_csv(\"LynxKite_export/LynxKite_outlier_viral_modeling_test.csv\", index_col=\"card_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lk_train.drop(['new_id', 'outlier', 'target', 'type'], inplace=True, axis=1)\n",
    "df_lk_test.drop(['new_id', 'outlier', 'target', 'type'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_lk_train, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_lk_test, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_hist_trans, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_hist_trans, on='card_id', how='left')\n",
    "\n",
    "df_train = pd.merge(df_train, df_new_trans, on='card_id', how='left')\n",
    "df_test = pd.merge(df_test, df_new_trans, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 212.97 MB\n",
      "Reduced memory usage: 127.67 MB (40.1% reduction)\n",
      "Starting memory usage: 128.51 MB\n",
      "Reduced memory usage: 81.70 MB (36.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier selection\n",
    "https://www.kaggle.com/sz8416/6-ways-for-feature-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marking the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['outlier'] = np.where(df_train['target']<-30, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,207 marked outliers in the training set.\n"
     ]
    }
   ],
   "source": [
    "print('There are {:,} marked outliers in the training set.'.format(len(df_train[df_train['outlier'] == 1]['outlier'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_train['outlier']\n",
    "\n",
    "use_cols = [c for c in df_train_wo_outliers.columns if c not in ['card_id', 'first_active_month', 'target', 'outlier', 'viral_outlier_test', 'viral_roles']]\n",
    "features = list(df_train_wo_outliers[use_cols].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, used_cols):\n",
    "    for c in df.columns:\n",
    "        if c in used_cols:\n",
    "            mean = statistics.mean(df[c])\n",
    "            std = statistics.stdev(df[c])\n",
    "\n",
    "            df[c] = (df[c] - mean)/std\n",
    "            print('{}: {:.4f} ({:.4f})'.format(c, mean, std))\n",
    "\n",
    "    return df[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean = df_train.dropna(how='any', axis=0, subset=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_1: 3.0869 (1.1955)\n",
      "feature_2: 1.7331 (0.7495)\n",
      "feature_3: 0.5537 (0.4971)\n",
      "elapsed_days: 361.6757 (286.1876)\n",
      "year: 2016.5540 (0.7591)\n",
      "month: 7.5097 (3.3249)\n",
      "days_feature1: 1154.0387 (1084.1325)\n",
      "days_feature2: 659.3918 (709.8126)\n",
      "days_feature3: 226.5280 (318.7878)\n",
      "number_of_transactions_x: 101.1063 (113.3242)\n",
      "merch_seg_viral_outlier_average_after_iteration_1_most_common: 0.0031 (0.0352)\n",
      "merch_seg_viral_outlier_average_after_iteration_2_most_common: 0.0020 (0.0270)\n",
      "merch_seg_viral_outlier_average_after_iteration_3_most_common: 0.0020 (0.0270)\n",
      "merch_seg_viral_outlier_average_after_iteration_4_most_common: 0.0020 (0.0270)\n",
      "merch_seg_viral_outlier_average_after_iteration_5_most_common: 0.0020 (0.0270)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_1_most_common: 0.0101 (0.0470)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_2_most_common: 0.0077 (0.0370)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_3_most_common: 0.0077 (0.0371)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_4_most_common: 0.0077 (0.0371)\n",
      "merch_seg_viral_outlier_standard_deviation_after_iteration_5_most_common: 0.0077 (0.0371)\n",
      "number_of_transactions_y: 101.1063 (113.3242)\n",
      "viral_outlier_after_iteration_1: 0.0076 (0.0867)\n",
      "viral_outlier_after_iteration_2: 0.0076 (0.0867)\n",
      "viral_outlier_after_iteration_3: 0.0076 (0.0867)\n",
      "viral_outlier_after_iteration_4: 0.0076 (0.0867)\n",
      "viral_outlier_after_iteration_5: 0.0076 (0.0867)\n",
      "viral_outlier_spread_over_iterations: 0.0000 (0.0000)\n",
      "viral_outlier_train: 0.0076 (0.0867)\n",
      "hist_transactions_count: 1.0000 (0.0000)\n",
      "hist_authorized_flag_sum: 93.1550 (107.8233)\n",
      "hist_authorized_flag_mean: 0.9031 (0.1021)\n",
      "hist_active_months_lag3_sum: 301.7536 (338.5628)\n",
      "hist_active_months_lag3_mean: 2.9997 (0.0050)\n",
      "hist_active_months_lag6_sum: 603.3500 (676.9353)\n",
      "hist_active_months_lag6_mean: 5.9979 (0.0194)\n",
      "hist_active_months_lag12_sum: 1182.8039 (1331.1210)\n",
      "hist_active_months_lag12_mean: 11.7385 (0.3116)\n",
      "hist_avg_sales_lag3_sum: 13103.8984 (153611.0326)\n",
      "hist_avg_sales_lag3_mean: 135.4573 (2036.1845)\n",
      "hist_avg_sales_lag6_sum: 12303.4147 (141587.8082)\n",
      "hist_avg_sales_lag6_mean: 127.0225 (1873.6306)\n",
      "hist_avg_sales_lag12_sum: 11322.1033 (132348.7354)\n",
      "hist_avg_sales_lag12_mean: 116.5618 (1727.7349)\n",
      "hist_avg_purchases_lag3_sum: inf (nan)\n",
      "hist_avg_purchases_lag3_mean: inf (nan)\n",
      "hist_avg_purchases_lag6_sum: inf (nan)\n",
      "hist_avg_purchases_lag6_mean: inf (nan)\n",
      "hist_avg_purchases_lag12_sum: inf (nan)\n",
      "hist_avg_purchases_lag12_mean: inf (nan)\n",
      "hist_card_id_size: 101.1063 (113.3242)\n",
      "hist_category_1_trans_sum: 5.8501 (17.9482)\n",
      "hist_category_1_trans_mean: 0.0715 (0.1455)\n",
      "hist_category_1_merch_sum: 12.6619 (21.6622)\n",
      "hist_category_1_merch_mean: 0.1443 (0.1592)\n",
      "hist_category_2_trans_sum: 204.9980 (314.7172)\n",
      "hist_category_2_trans_mean: 2.1674 (1.3709)\n",
      "hist_category_2_merch_sum: 197.1323 (308.0174)\n",
      "hist_category_2_merch_mean: 2.2531 (1.4798)\n",
      "hist_category_3_sum: 52.4541 (90.8074)\n",
      "hist_category_3_mean: 0.6287 (0.6043)\n",
      "hist_category_4_sum: 51.8784 (85.1249)\n",
      "hist_category_4_mean: 0.4950 (0.3768)\n",
      "hist_city_id_trans_nunique: 5.3882 (3.7128)\n",
      "hist_city_id_merch_nunique: 3.9474 (2.7032)\n",
      "hist_installments_sum: 63.0908 (108.4027)\n",
      "hist_installments_median: 0.5826 (0.7031)\n",
      "hist_installments_mean: 0.8056 (1.0568)\n",
      "hist_installments_max: 4.2801 (23.5477)\n",
      "hist_installments_min: -0.0034 (0.7142)\n",
      "hist_installments_std: 0.7588 (3.0763)\n",
      "hist_merchant_id_nunique: 38.6327 (32.7678)\n",
      "hist_merchant_category_id_trans_nunique: 20.2871 (11.7209)\n",
      "hist_merchant_category_id_merch_nunique: 19.4611 (11.0969)\n",
      "hist_merchant_group_id_nunique: 26.4757 (22.5131)\n",
      "hist_month_lag_max: -0.1276 (0.5727)\n",
      "hist_month_lag_min: -7.5717 (3.8888)\n",
      "hist_month_lag_mean: -3.6237 (2.0849)\n",
      "hist_month_lag_var: 6.7728 (6.0637)\n",
      "hist_most_recent_sales_range_sum: 230.2825 (267.7749)\n",
      "hist_most_recent_sales_range_mean: 2.2829 (0.5210)\n",
      "hist_most_recent_sales_range_max: 3.9317 (0.2898)\n",
      "hist_most_recent_sales_range_min: 0.0954 (0.3559)\n",
      "hist_most_recent_sales_range_std: 1.2369 (0.2309)\n",
      "hist_most_recent_purchases_range_sum: 238.3828 (281.4700)\n",
      "hist_most_recent_purchases_range_mean: 2.3220 (0.5245)\n",
      "hist_most_recent_purchases_range_max: 3.9297 (0.2993)\n",
      "hist_most_recent_purchases_range_min: 0.1024 (0.3593)\n",
      "hist_most_recent_purchases_range_std: 1.2228 (0.2275)\n",
      "hist_numerical_1_mean: 5.0999 (8.4840)\n",
      "hist_numerical_1_median: 0.3737 (4.3941)\n",
      "hist_numerical_1_max: 57.8367 (65.2488)\n",
      "hist_numerical_1_min: -0.0548 (0.2805)\n",
      "hist_numerical_1_std: 12.4801 (15.6679)\n",
      "hist_numerical_2_mean: 4.9852 (8.3709)\n",
      "hist_numerical_2_median: 0.3130 (4.3483)\n",
      "hist_numerical_2_max: 57.2100 (64.6092)\n",
      "hist_numerical_2_min: -0.0551 (0.2780)\n",
      "hist_numerical_2_std: 12.3616 (15.5284)\n",
      "hist_number_of_transactions_mean: 65824.6315 (71689.8357)\n",
      "hist_number_of_transactions_median: 8772.7998 (52968.6723)\n",
      "hist_number_of_transactions_max: 825648.9317 (457364.2805)\n",
      "hist_number_of_transactions_min: 29.6672 (545.0853)\n",
      "hist_number_of_transactions_std: 181342.1437 (126315.4475)\n",
      "hist_state_id_trans_nunique: 2.8788 (1.5846)\n",
      "hist_state_id_merch_nunique: 2.6343 (1.2554)\n",
      "hist_subsector_id_trans_nunique: 12.4157 (5.0694)\n",
      "hist_subsector_id_merch_nunique: 12.4341 (5.0492)\n",
      "hist_purchase_amount_sum: -23.6428 (1326.1740)\n",
      "hist_purchase_amount_mean: -0.1587 (20.5220)\n",
      "hist_purchase_amount_max: 35.9371 (1317.7344)\n",
      "hist_purchase_amount_min: -0.7333 (0.0465)\n",
      "hist_purchase_amount_std: 3.4999 (125.9451)\n",
      "hist_purchase_year_mean: 2017.1315 (0.3379)\n",
      "hist_purchase_year_median: 2017.1315 (0.3379)\n",
      "hist_purchase_year_max: 2017.7626 (0.4255)\n",
      "hist_purchase_year_min: 2017.0000 (0.0000)\n",
      "hist_purchase_year_std: 0.3064 (0.1884)\n",
      "hist_purchase_year_nunique: 1.7626 (0.4255)\n",
      "hist_purchase_month_mean: 6.5178 (1.8476)\n",
      "hist_purchase_month_median: 6.7252 (3.0031)\n",
      "hist_purchase_month_max: 11.2202 (1.8813)\n",
      "hist_purchase_month_min: 1.5525 (1.6570)\n",
      "hist_purchase_month_std: 3.4617 (1.1858)\n",
      "hist_purchase_month_nunique: 7.3595 (3.1404)\n",
      "hist_purchase_day_mean: 15.9745 (2.5634)\n",
      "hist_purchase_day_median: 16.0431 (3.9672)\n",
      "hist_purchase_day_max: 29.8820 (2.1890)\n",
      "hist_purchase_day_min: 2.0127 (2.2502)\n",
      "hist_purchase_day_std: 8.2786 (1.4021)\n",
      "hist_purchase_day_nunique: 21.7235 (7.9059)\n",
      "hist_purchase_hour_mean: 14.0797 (1.4907)\n",
      "hist_purchase_hour_median: 14.5865 (1.9618)\n",
      "hist_purchase_hour_max: 22.0756 (1.5019)\n",
      "hist_purchase_hour_min: 1.9164 (3.5129)\n",
      "hist_purchase_hour_std: 5.0884 (1.3065)\n",
      "hist_purchase_hour_nunique: 15.6550 (4.6622)\n",
      "hist_purchase_weekofyear_mean: 26.5200 (8.0181)\n",
      "hist_purchase_weekofyear_median: 27.3392 (12.9165)\n",
      "hist_purchase_weekofyear_max: 48.2228 (8.0570)\n",
      "hist_purchase_weekofyear_min: 4.0571 (7.5071)\n",
      "hist_purchase_weekofyear_std: 15.0618 (5.1392)\n",
      "hist_purchase_weekofyear_nunique: 21.4877 (12.5918)\n",
      "hist_purchase_dayofweek_mean: 3.0262 (0.5361)\n",
      "hist_purchase_dayofweek_median: 3.1330 (0.8612)\n",
      "hist_purchase_dayofweek_max: 5.8634 (0.4300)\n",
      "hist_purchase_dayofweek_min: 0.0715 (0.3349)\n",
      "hist_purchase_dayofweek_std: 1.8442 (0.2390)\n",
      "hist_purchase_dayofweek_nunique: 6.6303 (0.8332)\n",
      "hist_purchase_quarter_mean: 2.5175 (0.5905)\n",
      "hist_purchase_quarter_median: 2.5809 (1.0280)\n",
      "hist_purchase_quarter_max: 3.8182 (0.5436)\n",
      "hist_purchase_quarter_min: 1.1327 (0.4750)\n",
      "hist_purchase_quarter_std: 1.0740 (0.3437)\n",
      "hist_purchase_part_of_month_mean: 1.1042 (0.2440)\n",
      "hist_purchase_part_of_month_median: 1.1186 (0.4630)\n",
      "hist_purchase_weekend_sum: 28.4436 (34.3778)\n",
      "hist_purchase_weekend_mean: 0.2805 (0.1341)\n",
      "new_transactions_count: 1.0000 (0.0000)\n",
      "new_authorized_flag_sum: 7.9527 (6.8033)\n",
      "new_authorized_flag_mean: 1.0000 (0.0000)\n",
      "new_active_months_lag3_sum: 23.5637 (20.2815)\n",
      "new_active_months_lag3_mean: 2.9998 (0.0071)\n",
      "new_active_months_lag6_sum: 47.0617 (40.5024)\n",
      "new_active_months_lag6_mean: 5.9922 (0.0553)\n",
      "new_active_months_lag12_sum: 92.5879 (80.0955)\n",
      "new_active_months_lag12_mean: 11.7626 (0.4894)\n",
      "new_avg_sales_lag3_sum: 350.8786 (16869.8927)\n",
      "new_avg_sales_lag3_mean: 39.7449 (2460.9909)\n",
      "new_avg_sales_lag6_sum: 355.8204 (16697.4540)\n",
      "new_avg_sales_lag6_mean: 38.5351 (2252.7277)\n",
      "new_avg_sales_lag12_sum: 343.1875 (17449.0907)\n",
      "new_avg_sales_lag12_mean: 35.5980 (2034.5411)\n",
      "new_avg_purchases_lag3_sum: 35.6104 (1209.9060)\n",
      "new_avg_purchases_lag3_mean: 4.2728 (177.2859)\n",
      "new_avg_purchases_lag6_sum: 64.5491 (1108.4605)\n",
      "new_avg_purchases_lag6_mean: 8.4920 (163.0068)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_avg_purchases_lag12_sum: 67.6129 (1005.4959)\n",
      "new_avg_purchases_lag12_mean: 8.9693 (148.3317)\n",
      "new_card_id_size: 7.9527 (6.8033)\n",
      "new_category_1_trans_sum: 0.2218 (0.6011)\n",
      "new_category_1_trans_mean: 0.0323 (0.0950)\n",
      "new_category_1_merch_sum: 0.6690 (0.9932)\n",
      "new_category_1_merch_mean: 0.0963 (0.1479)\n",
      "new_category_2_trans_sum: 16.6452 (20.1448)\n",
      "new_category_2_trans_mean: 2.1821 (1.3982)\n",
      "new_category_2_merch_sum: 15.9578 (19.7995)\n",
      "new_category_2_merch_mean: 2.2387 (1.4763)\n"
     ]
    }
   ],
   "source": [
    "X = normalize(df_train_clean, features)\n",
    "#X = df_train_clean[features]\n",
    "y = df_train_clean['outlier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_selector(X, y, limit=100):\n",
    "    cor_list = []\n",
    "    for i in X.columns.tolist():\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "\n",
    "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-limit:]].columns.tolist()\n",
    "    cor_support = [True if i in cor_feature else False for i in X.columns.tolist()]\n",
    "    return cor_support, cor_feature, cor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2400: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2401: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2320: RuntimeWarning: invalid value encountered in subtract\n",
      "  X -= avg[:, None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 selected features\n"
     ]
    }
   ],
   "source": [
    "cor_support, cor_feature, cor_value = cor_selector(X, y)\n",
    "print(str(len(cor_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-945cccc9a217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mchi_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mchi_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    349\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m    350\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                         force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 573\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "X_norm = MinMaxScaler().fit_transform(X)\n",
    "chi_selector = SelectKBest(chi2, k=100)\n",
    "chi_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chi_selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5ee3c482d47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchi_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchi_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchi_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchi_support\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchi_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selected features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chi_selector' is not defined"
     ]
    }
   ],
   "source": [
    "chi_support = chi_selector.get_support()\n",
    "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "print(str(len(chi_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=5, step=10, verbose=5)\n",
    "rfe_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), '1.25*median')\n",
    "embeded_lr_selector.fit(X_norm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lr_support = embeded_lr_selector.get_support()\n",
    "embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "print(str(len(embeded_lr_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=5), threshold='1.25*median')\n",
    "embeded_rf_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_rf_support = embeded_rf_selector.get_support()\n",
    "embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "print(str(len(embeded_rf_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "\n",
    "embeded_lgb_selector = SelectFromModel(lgbc, threshold='1.25*median')\n",
    "embeded_lgb_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "print(str(len(embeded_lgb_feature)), 'selected features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "feature_selection_df = pd.DataFrame({\n",
    "    'feature': X.columns.tolist(),\n",
    "    'Pearson': cor_support,\n",
    "    'Chi-2': chi_support,\n",
    "    'RFE': rfe_support,\n",
    "    'Logistics': embeded_lr_support,\n",
    "    'Random Forest': embeded_rf_support,\n",
    "    'LightGBM': embeded_lgb_support\n",
    "})\n",
    "\n",
    "feature_selection_df['total'] = np.sum(feature_selection_df, axis=1)\n",
    "\n",
    "feature_selection_df = feature_selection_df.sort_values(['total', 'feature'] , ascending=False)\n",
    "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "feature_selection_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_selection_df[feature_selection_df['total'] > 3]['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(feature_selection_df[feature_selection_df['total'] > 3]['feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features used for training the outlier detector\n",
    "features = ['new_purchase_weekofyear_max', 'hist_category_3_sum', 'new_purchase_weekofyear_std', 'new_purchase_weekofyear_mean', 'new_purchase_day_std', 'new_purchase_day_median', 'new_purchase_day_mean', 'new_purchase_day_max', 'new_number_of_transactions_median', 'new_number_of_transactions_mean', 'new_month_lag_mean', 'hist_purchase_weekofyear_std', 'hist_month_lag_min', 'hist_installments_sum', 'hist_category_1_trans_sum', 'hist_category_1_trans_mean', 'hist_category_1_merch_sum', 'hist_category_1_merch_mean', 'hist_authorized_flag_mean', 'new_purchase_year_min', 'new_purchase_weekofyear_min', 'new_purchase_weekofyear_median', 'new_purchase_part_of_month_mean', 'new_purchase_month_std', 'new_purchase_month_mean', 'new_purchase_hour_mean', 'new_purchase_dayofweek_std', 'new_purchase_day_min', 'new_purchase_amount_sum', 'new_most_recent_purchases_range_sum', 'new_category_2_merch_sum', 'new_category_1_trans_sum', 'new_category_1_trans_mean', 'month', 'hist_purchase_weekofyear_min', 'hist_purchase_quarter_std', 'hist_purchase_month_std', 'hist_purchase_month_max', 'hist_number_of_transactions_median', 'hist_category_4_mean', 'hist_category_3_mean']\n",
    "X = df_train[features]\n",
    "y = df_train.outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(verbose=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_clean = df_test.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "randomforest_outlier_pred = clf.predict(df_test_clean[all_trainable_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomforest_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,136. card_id: C_ID_a8776b3483\n",
      "4,729. card_id: C_ID_6237a7c2b7\n",
      "5,160. card_id: C_ID_fc54efbb79\n",
      "9,158. card_id: C_ID_912688a50a\n",
      "12,343. card_id: C_ID_46eac83bde\n",
      "13,481. card_id: C_ID_1076351773\n",
      "14,872. card_id: C_ID_aae50409e7\n",
      "23,526. card_id: C_ID_ac114ef831\n",
      "32,867. card_id: C_ID_93c6231450\n",
      "36,467. card_id: C_ID_84e2faf848\n",
      "40,482. card_id: C_ID_e3c0c1325b\n",
      "41,393. card_id: C_ID_9771b57a38\n",
      "46,329. card_id: C_ID_0a21dd3613\n",
      "63,803. card_id: C_ID_fdf63e6d6e\n",
      "80,056. card_id: C_ID_aa32aa22e5\n",
      "86,906. card_id: C_ID_cab853c010\n",
      "87,384. card_id: C_ID_4cdcd7bbb1\n"
     ]
    }
   ],
   "source": [
    "rf_outlier_card_ids = []\n",
    "for i in range(len(randomforest_outlier_pred)):\n",
    "    if randomforest_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        rf_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C_ID_a8776b3483',\n",
       " 'C_ID_6237a7c2b7',\n",
       " 'C_ID_fc54efbb79',\n",
       " 'C_ID_912688a50a',\n",
       " 'C_ID_46eac83bde',\n",
       " 'C_ID_1076351773',\n",
       " 'C_ID_aae50409e7',\n",
       " 'C_ID_ac114ef831',\n",
       " 'C_ID_93c6231450',\n",
       " 'C_ID_84e2faf848',\n",
       " 'C_ID_e3c0c1325b',\n",
       " 'C_ID_9771b57a38',\n",
       " 'C_ID_0a21dd3613',\n",
       " 'C_ID_fdf63e6d6e',\n",
       " 'C_ID_aa32aa22e5',\n",
       " 'C_ID_cab853c010',\n",
       " 'C_ID_4cdcd7bbb1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_outlier_card_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(verbose=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_outlier_pred = clf.predict(df_test_clean[all_trainable_cols].dropna(axis=0, how='any'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34,464. card_id: C_ID_1fa5e84ac3\n",
      "47,151. card_id: C_ID_a5abf9fbc6\n",
      "61,984. card_id: C_ID_2df75addf1\n",
      "70,178. card_id: C_ID_2980922f6e\n"
     ]
    }
   ],
   "source": [
    "lr_outlier_card_ids = []\n",
    "for i in range(len(logistic_regression_outlier_pred)):\n",
    "    if logistic_regression_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        lr_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_outlier_pred = clf.predict(df_test_clean[all_trainable_cols].dropna(axis=0, how='any'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_outlier_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,801. card_id: C_ID_489ac69e52\n",
      "4,482. card_id: C_ID_4f5877bc71\n",
      "4,974. card_id: C_ID_b7e0a64084\n",
      "7,861. card_id: C_ID_4e8f2b5d1f\n",
      "8,100. card_id: C_ID_25df4fa561\n",
      "11,659. card_id: C_ID_9c760806b5\n",
      "14,872. card_id: C_ID_aae50409e7\n",
      "18,347. card_id: C_ID_159d993d5e\n",
      "19,834. card_id: C_ID_e4a5ea82d4\n",
      "28,012. card_id: C_ID_a8bc17bac9\n",
      "30,817. card_id: C_ID_055de6e0ad\n",
      "32,593. card_id: C_ID_bc1b6ba5c2\n",
      "39,928. card_id: C_ID_761c27a0f2\n",
      "40,296. card_id: C_ID_48f81f1eda\n",
      "42,657. card_id: C_ID_e94aaeede0\n",
      "44,918. card_id: C_ID_f2087398f1\n",
      "53,081. card_id: C_ID_ed948f3ebc\n",
      "53,321. card_id: C_ID_e895e3aaf4\n",
      "60,393. card_id: C_ID_01103857f9\n",
      "62,456. card_id: C_ID_ad43f734ab\n",
      "67,919. card_id: C_ID_efbf650295\n",
      "68,911. card_id: C_ID_8aa970734d\n",
      "76,734. card_id: C_ID_4f47eac613\n",
      "77,408. card_id: C_ID_ff19b99804\n",
      "79,336. card_id: C_ID_bc6e652855\n",
      "79,996. card_id: C_ID_d596794318\n",
      "89,153. card_id: C_ID_5ddfd1c1c4\n"
     ]
    }
   ],
   "source": [
    "adaboost_outlier_card_ids = []\n",
    "for i in range(len(adaboost_outlier_pred)):\n",
    "    if adaboost_outlier_pred[i] == 1:\n",
    "        print('{:,}. card_id: {}'.format(i, df_test_clean['card_id'].iloc[i]))\n",
    "        adaboost_outlier_card_ids.append(df_test_clean['card_id'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "clf = GradientBoostingRegressor({\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_split': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    'loss': 'ls'\n",
    "})\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_outlier_pred = clf.predict(df_test_clean[all_trainable_cols].dropna(axis=0, how='any'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the output of the logistic regression: 4\n",
      "Size of the output of the random forest: 17\n",
      "Size of the output of the random forest: 27\n"
     ]
    }
   ],
   "source": [
    "print('Size of the output of the logistic regression: {}\\nSize of the output of the random forest: {}\\nSize of the output of the random forest: {}'.format(len(lr_outlier_card_ids), len(rf_outlier_card_ids), len(adaboost_outlier_card_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(lr_outlier_card_ids, rf_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(lr_outlier_card_ids, adaboost_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C_ID_aae50409e7']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(rf_outlier_card_ids, adaboost_outlier_card_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.Booster(model_file='models/lightgbm_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = ['card_id', 'first_active_month', 'target', 'outlier']\n",
    "use_cols = [c for c in df_train.columns if c not in drops]\n",
    "features = list(df_train[use_cols].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(df_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "    \"card_id\": df_test[\"card_id\"].values\n",
    "})\n",
    "df_sub[\"target\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the intersection of RF & AdaBoost\n",
    "df_sub.loc[df_sub['card_id'] == 'C_ID_aae50409e7', 'target'] = -33.218750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of C_ID_1fa5e84ac3 is -5.515564316338457\n",
      "The value of C_ID_a5abf9fbc6 is -3.9438269947352578\n",
      "The value of C_ID_2df75addf1 is -5.361454488823543\n",
      "The value of C_ID_2980922f6e is -6.559434848065991\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lr_outlier_card_ids)):\n",
    "    print('The value of {} is {}'.format(lr_outlier_card_ids[i], df_sub.loc[df_sub['card_id'] == lr_outlier_card_ids[i], 'target'].values[0]))\n",
    "    df_sub.loc[df_sub['card_id'] == lr_outlier_card_ids[i], 'target'] = -33.218750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sub[df_sub['target'] < -30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"output/lgbm_rf_and_adaboost_outliers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random Forest (LB score: 5.994)\n",
    "* Logistic Regression (LB score: 5.990)\n",
    "* Random Forest & AdaBoost (LB score: 5.986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.969px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
