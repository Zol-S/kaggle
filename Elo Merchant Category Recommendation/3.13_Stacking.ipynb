{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Merchant Category Recommendation - Stacking\n",
    "End date: _2019. february 19._<br/>\n",
    "\n",
    "This tutorial notebook is part of a series for [Elo Mechant Category Recommendation](https://www.kaggle.com/c/elo-merchant-category-recommendation) contest organized by Elo, one of the largest payment brands in Brazil. It has built partnerships with merchants in order to offer promotions or discounts to cardholders. The objective of the competition is to identify and serve the most relevant opportunities to individuals, by uncovering signals in customer loyalty. LynxKite does not yet support some of the data preprocessing, thus they need to be done in Python. The input files are available from the [download](https://www.kaggle.com/c/elo-merchant-category-recommendation/data) section of the contest:\n",
    "\n",
    "- **train.csv**,  **test.csv**: list of `card_ids` that can be used for training and testing\n",
    "- **historical_transactions.csv**: contains up to 3 months' worth of transactions for every card at any of the provided `merchant_ids`\n",
    "- **new_merchant_transactions.csv**: contains the transactions at new merchants (`merchant_ids` that this particular `card_id` \n",
    "has not yet visited) over a period of two months\n",
    "- **merchants.csv**: contains aggregate information for each `merchant_id` represented in the data set\n",
    "\n",
    "Inspired by [MultiModel + RIDGE + STACKING](https://www.kaggle.com/ashishpatel26/rmse-3-66-multimodel-ridge-stacking)<br/>\n",
    "[KazaNova's presentation on stacking](https://www.coursera.org/lecture/competitive-data-science/stacking-Qdtt6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedKFold, KFold, train_test_split\n",
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, accuracy_score\n",
    "\n",
    "random.seed(1)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Starting memory usage: {:5.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min >= np.iinfo(np.int64).min and c_max <= np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min >= np.finfo(np.float16).min and c_max <= np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min >= np.finfo(np.float32).min and c_max <= np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Reduced memory usage: {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  9.24 MB\n",
      "Reduced memory usage:  4.04 MB (56.2% reduction)\n",
      "201,917 observations and 6 features in train set.\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"input/train.csv\", parse_dates=[\"first_active_month\"])\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_train.shape[0], df_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  4.72 MB\n",
      "Reduced memory usage:  2.24 MB (52.5% reduction)\n",
      "123,623 observations and 5 features in test set.\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"input/test.csv\", parse_dates=[\"first_active_month\"])\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "print(\"{:,} observations and {} features in test set.\".format(df_test.shape[0], df_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['first_active_month'] = pd.to_datetime(df_train['first_active_month'])\n",
    "df_train['elapsed_days'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['first_active_month'] = pd.to_datetime(df_test['first_active_month'])\n",
    "df_test['elapsed_days'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 3109.54 MB\n",
      "Reduced memory usage: 1749.11 MB (43.7% reduction)\n",
      "Number of historical transactions: 29,112,361\n",
      "CPU times: user 1min 13s, sys: 42.9 s, total: 1min 56s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_hist_trans = pd.read_csv('input/historical_transactions.csv', parse_dates=['purchase_date'])\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "print('Number of historical transactions: {:,}'.format(len(df_hist_trans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 209.67 MB\n",
      "Reduced memory usage: 114.20 MB (45.5% reduction)\n",
      "Number of new transactions: 1,963,031\n",
      "CPU times: user 5.03 s, sys: 625 ms, total: 5.66 s\n",
      "Wall time: 6.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_new_trans = pd.read_csv('input/new_merchant_transactions.csv', parse_dates=['purchase_date'])\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "print('Number of new transactions: {:,}'.format(len(df_new_trans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authorized_flag              0\n",
       "card_id                      0\n",
       "city_id                      0\n",
       "category_1                   0\n",
       "installments                 0\n",
       "category_3               55922\n",
       "merchant_category_id         0\n",
       "merchant_id              26216\n",
       "month_lag                    0\n",
       "purchase_amount              0\n",
       "purchase_date                0\n",
       "category_2              111745\n",
       "state_id                     0\n",
       "subsector_id                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new_trans.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data handling (why these values?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_hist_trans, df_new_trans]:\n",
    "    df['category_2'].fillna(1.0, inplace=True)\n",
    "    df['category_3'].fillna('A', inplace=True)\n",
    "    df['merchant_id'].fillna('M_ID_00a6ca8a8a', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_features(df, source_column, preposition):\n",
    "    df[preposition + '_year'] = df[source_column].dt.year\n",
    "    df[preposition + '_month'] = df[source_column].dt.month\n",
    "    df[preposition + '_day'] = df[source_column].dt.day\n",
    "    df[preposition + '_hour'] = df[source_column].dt.hour\n",
    "    df[preposition + '_weekofyear'] = df[source_column].dt.weekofyear\n",
    "    df[preposition + '_dayofweek'] = df[source_column].dt.dayofweek\n",
    "    df[preposition + '_weekend'] = (df[source_column].dt.weekday >=5).astype(int)\n",
    "    df[preposition + '_quarter'] = df[source_column].dt.quarter\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans['authorized_flag'] = df_hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "df_hist_trans['category_1'] = df_hist_trans['category_1'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "df_hist_trans['purchase_date'] = pd.to_datetime(df_hist_trans['purchase_date'])\n",
    "df_hist_trans = create_date_features(df_hist_trans, 'purchase_date', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['authorized_flag'] = df_new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\n",
    "df_new_trans['category_1'] = df_new_trans['category_1'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "df_new_trans['purchase_date'] = pd.to_datetime(df_new_trans['purchase_date'])\n",
    "df_new_trans = create_date_features(df_new_trans, 'purchase_date', 'purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans['month_diff'] = ((datetime.date(2018, 12, 1) - df_hist_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_hist_trans['month_diff'] += df_hist_trans['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['month_diff'] = ((datetime.date(2018, 12, 1) - df_new_trans['purchase_date'].dt.date).dt.days)//30\n",
    "df_new_trans['month_diff'] += df_new_trans['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 3748.10 MB\n",
      "Reduced memory usage: 1638.06 MB (56.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage: 248.99 MB\n",
      "Reduced memory usage: 106.71 MB (57.1% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_new_trans = reduce_mem_usage(df_new_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_transactions(df, prefix):\n",
    "    agg_funcs = {\n",
    "        'authorized_flag': ['sum', 'mean'],\n",
    "\n",
    "        'card_id': ['size'],\n",
    "        'category_1': ['sum', 'mean'],\n",
    "        \n",
    "        'installments': ['sum', 'max', 'min', 'mean', 'var'],\n",
    "\n",
    "        'merchant_category_id': ['nunique'],\n",
    "        'merchant_id': ['nunique'],\n",
    "        'month_diff': ['mean'],\n",
    "        'month_lag': ['max','min','mean','var'],\n",
    "\n",
    "        'purchase_amount': ['sum', 'max', 'min', 'mean', 'var'],\n",
    "        'purchase_date': ['max', 'min'],\n",
    "        'purchase_dayofweek': ['nunique'],\n",
    "        'purchase_hour': ['nunique'],\n",
    "        'purchase_month': ['nunique'],\n",
    "        'purchase_year': ['nunique'],\n",
    "        'purchase_weekend': ['sum', 'mean'],\n",
    "        'purchase_weekofyear': ['nunique'],\n",
    "\n",
    "        'subsector_id': ['nunique']\n",
    "    }\n",
    "\n",
    "    df['category_2_mean'] = df.groupby(['category_2'])['purchase_amount'].transform('mean')\n",
    "    df['category_3_mean'] = df.groupby(['category_3'])['purchase_amount'].transform('mean')\n",
    "\n",
    "    df_agg = df.groupby('card_id').agg(agg_funcs)\n",
    "    df_agg.columns = [prefix + '_' + '_'.join(col).strip() for col in df_agg.columns.values]\n",
    "    df_agg.reset_index(drop=False, inplace=True)\n",
    "    df_agg[prefix + '_purchase_date_diff'] = (df_agg[prefix + '_purchase_date_max'] - df_agg[prefix + '_purchase_date_min']).dt.days\n",
    "    df_agg[prefix + '_purchase_date_average'] = df_agg[prefix + '_purchase_date_diff']/df_agg[prefix + '_card_id_size']\n",
    "    df_agg[prefix + '_purchase_date_uptonow'] = (datetime.datetime.today() - df_agg[prefix + '_purchase_date_max']).dt.days\n",
    "\n",
    "    df = (df_agg.groupby('card_id').size().reset_index(name='{}_transactions_count'.format(prefix)))\n",
    "    df_agg = pd.merge(df, df_agg, on='card_id', how='left')\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_agg = aggregate_transactions(df_hist_trans, 'hist')\n",
    "df_new_agg = aggregate_transactions(df_new_trans, 'new')\n",
    "\n",
    "df_train = df_train.merge(df_hist_agg, on='card_id', how='left')\n",
    "df_train = df_train.merge(df_new_agg, on='card_id', how='left')\n",
    "\n",
    "df_test = df_test.merge(df_hist_agg, on='card_id', how='left')\n",
    "df_test = df_test.merge(df_new_agg, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card_id</th>\n",
       "      <th>hist_transactions_count</th>\n",
       "      <th>hist_authorized_flag_sum</th>\n",
       "      <th>hist_authorized_flag_mean</th>\n",
       "      <th>hist_card_id_size</th>\n",
       "      <th>hist_category_1_sum</th>\n",
       "      <th>hist_category_1_mean</th>\n",
       "      <th>hist_installments_sum</th>\n",
       "      <th>hist_installments_max</th>\n",
       "      <th>hist_installments_min</th>\n",
       "      <th>...</th>\n",
       "      <th>hist_purchase_hour_nunique</th>\n",
       "      <th>hist_purchase_month_nunique</th>\n",
       "      <th>hist_purchase_year_nunique</th>\n",
       "      <th>hist_purchase_weekend_sum</th>\n",
       "      <th>hist_purchase_weekend_mean</th>\n",
       "      <th>hist_purchase_weekofyear_nunique</th>\n",
       "      <th>hist_subsector_id_nunique</th>\n",
       "      <th>hist_purchase_date_diff</th>\n",
       "      <th>hist_purchase_date_average</th>\n",
       "      <th>hist_purchase_date_uptonow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_ID_00007093c1</td>\n",
       "      <td>1</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.765101</td>\n",
       "      <td>149</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.187919</td>\n",
       "      <td>192</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.167785</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>377</td>\n",
       "      <td>2.530201</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_ID_0001238066</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.975610</td>\n",
       "      <td>123</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>198</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.422764</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>151</td>\n",
       "      <td>1.227642</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_ID_0001506ef0</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>398</td>\n",
       "      <td>6.030303</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           card_id  hist_transactions_count  hist_authorized_flag_sum  \\\n",
       "0  C_ID_00007093c1                        1                     114.0   \n",
       "1  C_ID_0001238066                        1                     120.0   \n",
       "2  C_ID_0001506ef0                        1                      62.0   \n",
       "\n",
       "   hist_authorized_flag_mean  hist_card_id_size  hist_category_1_sum  \\\n",
       "0                   0.765101                149                 28.0   \n",
       "1                   0.975610                123                  2.0   \n",
       "2                   0.939394                 66                  0.0   \n",
       "\n",
       "   hist_category_1_mean  hist_installments_sum  hist_installments_max  \\\n",
       "0              0.187919                    192                      6   \n",
       "1              0.016260                    198                     10   \n",
       "2              0.000000                      1                      1   \n",
       "\n",
       "   hist_installments_min             ...              \\\n",
       "0                      1             ...               \n",
       "1                     -1             ...               \n",
       "2                      0             ...               \n",
       "\n",
       "   hist_purchase_hour_nunique  hist_purchase_month_nunique  \\\n",
       "0                          18                           12   \n",
       "1                          20                            6   \n",
       "2                          15                           11   \n",
       "\n",
       "   hist_purchase_year_nunique  hist_purchase_weekend_sum  \\\n",
       "0                           2                       25.0   \n",
       "1                           2                       52.0   \n",
       "2                           2                       32.0   \n",
       "\n",
       "   hist_purchase_weekend_mean  hist_purchase_weekofyear_nunique  \\\n",
       "0                    0.167785                                39   \n",
       "1                    0.422764                                23   \n",
       "2                    0.484848                                24   \n",
       "\n",
       "   hist_subsector_id_nunique  hist_purchase_date_diff  \\\n",
       "0                         13                      377   \n",
       "1                         17                      151   \n",
       "2                         12                      398   \n",
       "\n",
       "   hist_purchase_date_average  hist_purchase_date_uptonow  \n",
       "0                    2.530201                         363  \n",
       "1                    1.227642                         362  \n",
       "2                    6.030303                         373  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist_agg[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_hist_trans, df_new_trans, df_hist_agg, df_new_agg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['hist_first_buy'] = (df_train['hist_purchase_date_min'] - df_train['first_active_month']).dt.days\n",
    "df_train['new_first_buy'] = (df_train['new_purchase_date_min'] - df_train['first_active_month']).dt.days\n",
    "\n",
    "df_test['hist_first_buy'] = (df_test['hist_purchase_date_min'] - df_test['first_active_month']).dt.days\n",
    "df_test['new_first_buy'] = (df_test['new_purchase_date_min'] - df_test['first_active_month']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['hist_purchase_date_max', 'hist_purchase_date_min', 'new_purchase_date_max', 'new_purchase_date_min']:\n",
    "    df_train[f] = df_train[f].astype(np.int64) * 1e-9\n",
    "    df_test[f] = df_test[f].astype(np.int64) * 1e-9\n",
    "\n",
    "df_train['card_id_total'] = df_train['hist_card_id_size'] + df_train['new_card_id_size']\n",
    "df_test['card_id_total'] = df_test['hist_card_id_size'] + df_test['new_card_id_size']\n",
    "\n",
    "df_train['purchase_amount_total'] = df_train['hist_purchase_amount_sum'] + df_train['new_purchase_amount_sum']\n",
    "df_test['purchase_amount_total'] = df_test['hist_purchase_amount_sum'] + df_test['new_purchase_amount_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199710\n",
       "1      2207\n",
       "Name: outliers, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['feature_1', 'feature_2', 'feature_3']:\n",
    "    order_label = df_train.groupby([f])['outliers'].mean()\n",
    "    df_train[f] = df_train[f].map(order_label)\n",
    "    df_test[f] = df_test[f].map(order_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('preprocessed/train_merged.csv')\n",
    "df_test.to_csv('preprocessed/test_merged.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading possible outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  3.08 MB\n",
      "Reduced memory usage:  1.73 MB (43.7% reduction)\n",
      "201,917 observations and 2 features in train set.\n"
     ]
    }
   ],
   "source": [
    "df_train_po = pd.read_csv(\"preprocessed/possible_outliers_train_gt0.9.csv\")\n",
    "df_train_po = reduce_mem_usage(df_train_po)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_train_po.shape[0], df_train_po.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting memory usage:  1.89 MB\n",
      "Reduced memory usage:  1.06 MB (43.7% reduction)\n",
      "123,623 observations and 2 features in train set.\n"
     ]
    }
   ],
   "source": [
    "df_test_po = pd.read_csv(\"preprocessed/possible_outliers_test_gt0.9.csv\")\n",
    "df_test_po = reduce_mem_usage(df_test_po)\n",
    "print(\"{:,} observations and {} features in train set.\".format(df_test_po.shape[0], df_test_po.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31039, 335)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_po[df_train_po['possible_out'] == 1]), len(df_test_po[df_test_po['possible_out'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.merge(df_train_po, on='card_id', how='left')\n",
    "df_test = df_test.merge(df_test_po, on='card_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "[LightGBM parameter tuning](https://testlightgbm.readthedocs.io/en/latest/Parameters-tuning.html)<br/>\n",
    "[What is LightGBM, How to implement it? How to fine tune the parameters?](https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month', 'target', 'outliers']]\n",
    "target = df_train['target']\n",
    "del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, y_train, y_valid = train_test_split(df_train, target, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.65059\tvalid_1's rmse: 3.77314\n",
      "[200]\ttraining's rmse: 3.57194\tvalid_1's rmse: 3.73943\n",
      "[300]\ttraining's rmse: 3.52137\tvalid_1's rmse: 3.72711\n",
      "[400]\ttraining's rmse: 3.48248\tvalid_1's rmse: 3.71966\n",
      "[500]\ttraining's rmse: 3.44988\tvalid_1's rmse: 3.71506\n",
      "[600]\ttraining's rmse: 3.42262\tvalid_1's rmse: 3.71157\n",
      "[700]\ttraining's rmse: 3.39801\tvalid_1's rmse: 3.70892\n",
      "[800]\ttraining's rmse: 3.37715\tvalid_1's rmse: 3.70651\n",
      "[900]\ttraining's rmse: 3.35593\tvalid_1's rmse: 3.70401\n",
      "[1000]\ttraining's rmse: 3.33781\tvalid_1's rmse: 3.7031\n",
      "[1100]\ttraining's rmse: 3.3196\tvalid_1's rmse: 3.70271\n",
      "[1200]\ttraining's rmse: 3.30242\tvalid_1's rmse: 3.70167\n",
      "[1300]\ttraining's rmse: 3.28483\tvalid_1's rmse: 3.70084\n",
      "[1400]\ttraining's rmse: 3.26958\tvalid_1's rmse: 3.70045\n",
      "[1500]\ttraining's rmse: 3.25469\tvalid_1's rmse: 3.70002\n",
      "[1600]\ttraining's rmse: 3.23934\tvalid_1's rmse: 3.69951\n",
      "[1700]\ttraining's rmse: 3.22369\tvalid_1's rmse: 3.69966\n",
      "Early stopping, best iteration is:\n",
      "[1614]\ttraining's rmse: 3.23707\tvalid_1's rmse: 3.69946\n",
      "\n",
      "Fold 2.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.68011\tvalid_1's rmse: 3.65422\n",
      "[200]\ttraining's rmse: 3.60025\tvalid_1's rmse: 3.62247\n",
      "[300]\ttraining's rmse: 3.54972\tvalid_1's rmse: 3.61127\n",
      "[400]\ttraining's rmse: 3.51071\tvalid_1's rmse: 3.60586\n",
      "[500]\ttraining's rmse: 3.47918\tvalid_1's rmse: 3.60223\n",
      "[600]\ttraining's rmse: 3.45022\tvalid_1's rmse: 3.59997\n",
      "[700]\ttraining's rmse: 3.42559\tvalid_1's rmse: 3.59804\n",
      "[800]\ttraining's rmse: 3.40279\tvalid_1's rmse: 3.5972\n",
      "[900]\ttraining's rmse: 3.38321\tvalid_1's rmse: 3.59644\n",
      "[1000]\ttraining's rmse: 3.36311\tvalid_1's rmse: 3.5957\n",
      "[1100]\ttraining's rmse: 3.34434\tvalid_1's rmse: 3.59564\n",
      "[1200]\ttraining's rmse: 3.32642\tvalid_1's rmse: 3.59521\n",
      "[1300]\ttraining's rmse: 3.30999\tvalid_1's rmse: 3.59542\n",
      "Early stopping, best iteration is:\n",
      "[1210]\ttraining's rmse: 3.32484\tvalid_1's rmse: 3.59514\n",
      "\n",
      "Fold 3.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.67713\tvalid_1's rmse: 3.66294\n",
      "[200]\ttraining's rmse: 3.59696\tvalid_1's rmse: 3.63282\n",
      "[300]\ttraining's rmse: 3.54766\tvalid_1's rmse: 3.62007\n",
      "[400]\ttraining's rmse: 3.51039\tvalid_1's rmse: 3.61346\n",
      "[500]\ttraining's rmse: 3.47833\tvalid_1's rmse: 3.60851\n",
      "[600]\ttraining's rmse: 3.45134\tvalid_1's rmse: 3.60506\n",
      "[700]\ttraining's rmse: 3.42665\tvalid_1's rmse: 3.60238\n",
      "[800]\ttraining's rmse: 3.4044\tvalid_1's rmse: 3.60147\n",
      "[900]\ttraining's rmse: 3.38232\tvalid_1's rmse: 3.6002\n",
      "[1000]\ttraining's rmse: 3.36304\tvalid_1's rmse: 3.59946\n",
      "[1100]\ttraining's rmse: 3.3449\tvalid_1's rmse: 3.59923\n",
      "Early stopping, best iteration is:\n",
      "[1018]\ttraining's rmse: 3.35961\tvalid_1's rmse: 3.59913\n",
      "\n",
      "Fold 4.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.66526\tvalid_1's rmse: 3.72136\n",
      "[200]\ttraining's rmse: 3.58574\tvalid_1's rmse: 3.6862\n",
      "[300]\ttraining's rmse: 3.53685\tvalid_1's rmse: 3.6715\n",
      "[400]\ttraining's rmse: 3.49914\tvalid_1's rmse: 3.66338\n",
      "[500]\ttraining's rmse: 3.46805\tvalid_1's rmse: 3.65836\n",
      "[600]\ttraining's rmse: 3.44063\tvalid_1's rmse: 3.65432\n",
      "[700]\ttraining's rmse: 3.41658\tvalid_1's rmse: 3.65225\n",
      "[800]\ttraining's rmse: 3.39472\tvalid_1's rmse: 3.65082\n",
      "[900]\ttraining's rmse: 3.37465\tvalid_1's rmse: 3.64939\n",
      "[1000]\ttraining's rmse: 3.35534\tvalid_1's rmse: 3.64784\n",
      "[1100]\ttraining's rmse: 3.3365\tvalid_1's rmse: 3.64716\n",
      "[1200]\ttraining's rmse: 3.31874\tvalid_1's rmse: 3.64631\n",
      "[1300]\ttraining's rmse: 3.30193\tvalid_1's rmse: 3.64558\n",
      "[1400]\ttraining's rmse: 3.28634\tvalid_1's rmse: 3.64479\n",
      "[1500]\ttraining's rmse: 3.27081\tvalid_1's rmse: 3.64444\n",
      "[1600]\ttraining's rmse: 3.2556\tvalid_1's rmse: 3.64443\n",
      "Early stopping, best iteration is:\n",
      "[1535]\ttraining's rmse: 3.26506\tvalid_1's rmse: 3.64427\n",
      "\n",
      "Fold 5.\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.64796\tvalid_1's rmse: 3.78002\n",
      "[200]\ttraining's rmse: 3.56618\tvalid_1's rmse: 3.75286\n",
      "[300]\ttraining's rmse: 3.51549\tvalid_1's rmse: 3.74547\n",
      "[400]\ttraining's rmse: 3.47739\tvalid_1's rmse: 3.73967\n",
      "[500]\ttraining's rmse: 3.4456\tvalid_1's rmse: 3.73714\n",
      "[600]\ttraining's rmse: 3.4184\tvalid_1's rmse: 3.73485\n",
      "[700]\ttraining's rmse: 3.39394\tvalid_1's rmse: 3.7338\n",
      "[800]\ttraining's rmse: 3.37247\tvalid_1's rmse: 3.73269\n",
      "[900]\ttraining's rmse: 3.35112\tvalid_1's rmse: 3.73238\n",
      "[1000]\ttraining's rmse: 3.33196\tvalid_1's rmse: 3.73243\n",
      "Early stopping, best iteration is:\n",
      "[950]\ttraining's rmse: 3.34181\tvalid_1's rmse: 3.73211\n",
      "CPU times: user 15min 21s, sys: 3.78 s, total: 15min 25s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param = {\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_seed\": 11,\n",
    "    \"boosting\": \"gbdt\",\n",
    "\n",
    "    \"feature_fraction\": 0.9,\n",
    "    #\"feature_fraction_seed\": 1,\n",
    "\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"learning_rate\": 0.01,\n",
    "\n",
    "    #\"max_bin\": 5,\n",
    "    'max_depth': -1,\n",
    "    \"metric\": \"rmse\",\n",
    "    #\"min_data_in_leaf\": 30,\n",
    "    #\"min_gain_to_split\": 0.1,\n",
    "    \"min_child_samples\": 20,\n",
    "    #\"num_boost_round\": 100,\n",
    "    \"num_leaves\": 31,\n",
    "    \"nthread\": -1,\n",
    "\n",
    "    \"objective\": \"regression\",\n",
    "\n",
    "    \"random_state\": 402,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=402)\n",
    "oof_lgbm = np.zeros(len(df_train))\n",
    "predictions_lgbm = np.zeros(len(df_test))\n",
    "feature_importance_lgbm = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train, df_train['possible_out'].values)):\n",
    "    print(\"\\nFold {}.\".format(fold_+1))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])\n",
    "\n",
    "    num_round = 10000\n",
    "    model_lgbm = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds=100)\n",
    "    oof_lgbm[val_idx] = model_lgbm.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=model_lgbm.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = model_lgbm.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_lgbm = pd.concat([feature_importance_lgbm, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_lgbm += model_lgbm.predict(df_test[df_train_columns], num_iteration=model_lgbm.best_iteration) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 3.654425\n"
     ]
    }
   ],
   "source": [
    "cv_score_lgbm = np.sqrt(mean_squared_error(oof_lgbm, target))\n",
    "print(\"CV score: {:.6f}\".format(cv_score_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm.save_model(\"models/lightgbm_{:.6f}.txt\".format(cv_score_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgbm = lgb.Booster(model_file='models/lightgbm_3.655203.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_lgbm[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "\n",
    "best_features_lgbm = feature_importance_lgbm.loc[feature_importance_lgbm.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14, 25))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features_lgbm.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1.\n",
      "[0]\ttrain-rmse:3.85013\tvalid-rmse:3.96109\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[29]\ttrain-rmse:2.94438\tvalid-rmse:3.73769\n",
      "\n",
      "\n",
      "Fold 2.\n",
      "[0]\ttrain-rmse:3.87887\tvalid-rmse:3.84677\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[23]\ttrain-rmse:3.05815\tvalid-rmse:3.6268\n",
      "\n",
      "\n",
      "Fold 3.\n",
      "[0]\ttrain-rmse:3.88369\tvalid-rmse:3.84029\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[27]\ttrain-rmse:3.00697\tvalid-rmse:3.6372\n",
      "\n",
      "\n",
      "Fold 4.\n",
      "[0]\ttrain-rmse:3.86431\tvalid-rmse:3.91374\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[36]\ttrain-rmse:2.88175\tvalid-rmse:3.6748\n",
      "\n",
      "\n",
      "Fold 5.\n",
      "[0]\ttrain-rmse:3.85041\tvalid-rmse:3.95968\n",
      "Multiple eval metrics have been passed: 'valid-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until valid-rmse hasn't improved in 50 rounds.\n",
      "Stopping. Best iteration:\n",
      "[22]\ttrain-rmse:3.03528\tvalid-rmse:3.75918\n",
      "\n",
      "CPU times: user 20min 4s, sys: 1.14 s, total: 20min 5s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eta': 0.01,\n",
    "    'eval_metric': 'rmse',\n",
    "    'max_depth': 20,\n",
    "    'objective': 'reg:linear',\n",
    "    'silent': False,\n",
    "    'subsample': 0.8\n",
    "}\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=402)\n",
    "oof_xgb = np.zeros(len(df_train))\n",
    "predictions_xgb = np.zeros(len(df_test))\n",
    "feature_importance_xgb = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train, df_train['possible_out'].values)):\n",
    "    print(\"\\nFold {}.\".format(fold_+1))\n",
    "    trn_data = xgb.DMatrix(data=df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])\n",
    "    val_data = xgb.DMatrix(data=df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])\n",
    "    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n",
    "\n",
    "    num_round = 10000\n",
    "    model_xgb = xgb.train(params, trn_data, num_round, watchlist, early_stopping_rounds=50, verbose_eval=1000)\n",
    "    oof_xgb[val_idx] = model_xgb.predict(xgb.DMatrix(df_train.iloc[val_idx][df_train_columns]), ntree_limit=model_xgb.best_ntree_limit+50)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = df_train_columns\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_lxgb = pd.concat([feature_importance_xgb, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions_xgb += model_xgb.predict(xgb.DMatrix(df_test[df_train_columns]), ntree_limit=model_xgb.best_ntree_limit+50) / folds.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 3.702524\n"
     ]
    }
   ],
   "source": [
    "cv_score_xgb = np.sqrt(mean_squared_error(oof_xgb, target))\n",
    "print(\"CV score: {:.6f}\".format(cv_score_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/xgboost_3.702524.pkl']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model_xgb, \"models/xgboost_{:.6f}.pkl\".format(cv_score_xgb), compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = joblib.load('models/xgboost_3.657233.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\n",
    "    \"card_id\": df_test[\"card_id\"].values\n",
    "})\n",
    "\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"output/xgboost_{:.6f}.csv\".format(cv_score), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1.\n",
      "----------Ridge Regression0----------\n",
      "\n",
      "Fold 2.\n",
      "----------Ridge Regression1----------\n",
      "\n",
      "Fold 3.\n",
      "----------Ridge Regression2----------\n",
      "\n",
      "Fold 4.\n",
      "----------Ridge Regression3----------\n",
      "\n",
      "Fold 5.\n",
      "----------Ridge Regression4----------\n"
     ]
    }
   ],
   "source": [
    "train_stack = np.vstack([oof_lgbm, oof_xgb]).transpose()\n",
    "test_stack = np.vstack([predictions_lgbm, predictions_xgb]).transpose()\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions_stack = np.zeros(test_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n",
    "    print(\"\\nFold {}.\".format(fold_+1))\n",
    "    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n",
    "    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n",
    "\n",
    "    model_ridge = Ridge(alpha=100)\n",
    "    model_ridge.fit(trn_data, trn_y)\n",
    "    \n",
    "    oof_stack[val_idx] = model_ridge.predict(val_data)\n",
    "    predictions_stack += model_ridge.predict(test_stack) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 3.653519\n"
     ]
    }
   ],
   "source": [
    "cv_score_stack = np.sqrt(mean_squared_error(oof_stack, target))\n",
    "print(\"CV score: {:.6f}\".format(cv_score_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\n",
    "    \"card_id\": df_test[\"card_id\"].values\n",
    "})\n",
    "\n",
    "sub_df[\"target\"] = predictions_stack\n",
    "sub_df.to_csv(\"output/stacked_{:.6f}.csv\".format(cv_score_stack), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Just Train Data - LGB & XGB & CatBoost w/ Blending](https://www.kaggle.com/silverstone1903/just-train-data-lgb-xgb-catboost-w-blending/data)<br/>\n",
    "[MultiModel + RIDGE + STACKING](https://www.kaggle.com/ashishpatel26/rmse-3-66-multimodel-ridge-stacking)\n",
    "https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/75935"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "189px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
